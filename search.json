[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HYLODE",
    "section": "",
    "text": "Visit the application!\n\n\n\nThis link will only work from within the UCLH network, and you will need to contact the development team for a username and password. The application is in beta but weâ€™re keen to find early testers for feedback.\n\n\n\n\n\n\n\n\nOther resources\n\n\n\n\n\n\nLink to the Teams meeting for the morning sitrep\nread the user documentation\nexplore the developer guide\nget involved or learn more about the project\ntry the alpha version! ðŸ˜¬ðŸ˜±\nuse the original sitrep\ncheckout the INFORM-US dashboard"
  },
  {
    "objectID": "developer/infrastructure.html",
    "href": "developer/infrastructure.html",
    "title": "HYLODE Infrastructure",
    "section": "",
    "text": "Our infrastructure is managed under the HySys repository which contains a series of readme files. Setup and deployment of the various services is described here.\nDetailed documentation for each major subcomponent of HySys is provided below:"
  },
  {
    "objectID": "developer/infrastructure.html#deployment-locations",
    "href": "developer/infrastructure.html#deployment-locations",
    "title": "HYLODE Infrastructure",
    "section": "Deployment locations",
    "text": "Deployment locations\nDeployment information for the prod and staging_red HySys deployments such as host, directory location on the host and assigned ports can be found in the HySys README.md."
  },
  {
    "objectID": "developer/setup.html",
    "href": "developer/setup.html",
    "title": "Setting up development for HyUi",
    "section": "",
    "text": "The root directory contains all the libraries that are used to power the HyUi dashboards.\n\napi: This is a FastAPI app that does all the data wrangling and accessing of external sources such as to databases and web services. For each route there should be an equivalent mock route that allows this app to serve mock data when run in development. For example, if there is a /census/ endpoint then there should be a corresponding /mock/census endpoint that only serves mock data.\nweb: This contains the Plotly Dash web server code. It should only communicate with the api service. This allows the api service to provide mock data seamlessly when developing without needing to spin up other services to help development.\nmodels: This contains all the Pydantic models that are used to share data between the api and web.\ninitialise: This sets-up the BaseRow instance that is used for configuration by the user.\ncache: This contains the HTTP cache service that serves cached objects from api to web with a 5 minute expiry. See the deployment docs for more information.\nbaserow: We use baserow as a temporary database to that allows us to store information about patients without directly editing EMAP or EPIC. It is not relevant when running HyUI on your local machine, as all mock data will be stored locally. However, it is important to understand when deploying HyUI with live data within the NHS.\n\n\n\n\nweb should only communicate with api for data. This allows us to entirely mock data for web when developing.\nCommunication between web and api should only be done in a defined way using models described in models. models is the only shared code between api and web.\nPages in web/src/web/pages should be independent of each other so there is minimal/no sharing between two sets of pages. Common code can be put in a separate module or package in web."
  },
  {
    "objectID": "developer/setup.html#local-development",
    "href": "developer/setup.html#local-development",
    "title": "Setting up development for HyUi",
    "section": "Local Development",
    "text": "Local Development\n\nInstall Requirements\n\nInstall pyenv\nInstall pyenv-virtualenv\n\n\n\n1. Clone the HyUi repository to your local machine.\ngit clone https://github.com/HYLODE/HyUi.git\n\n\n2. Create Required Virtual Environments\npyenv virtualenv 3.11.0 hyui-web\npyenv virtualenv 3.11.0 hyui-api\nAs of Feb 2023, Python 3.11.0 causes diskcache / SQLalchemy issues with local development on some machines. For now, we recommend changing the above commands to instead use python 3.10.8.\n\n\n3. Activate Virtual Environments\nIn one shell:\npyenv activate hyui-web\nIn another shell:\npyenv activate hyui-api\nThese will be the Python virtual environments that contain all the dependencies for the Dash web app and the API backend.\n\n\n4. Install the Website app requirements\nIn the hyui-web terminal in the root directory:\npip install -e \"models[test]\"\npip install -e \"web[test]\"\n\n\n5. Install the API requirements\nIn the hyui-api terminal in the root directory:\npip install -e \"models[test]\"\npip install -e \"api[test]\"\n\n\n6. Install the pre-commit Hooks\nIn your shell (hyui-api or hyui-web) run the following command:\npre-commit install\n\n\n6. Configure Environment Variables\nYou will need to create and configure a .env file in your directory root.\nAn example environment variable file can be found at .env.example. The environment variables for each app are documented in the Pydantic Settings classes in web/src/web/config.py and api/src/api/config.py. An important one for development and production is the WEB_API_URL variable. This should be set to the host and port of your api server so most likely something like http://api:8000 for running in Docker or http://localhost:[port]/mock for development. The port needs to line up with that specified when starting the FastAPI server below (we have used 8092 throughout these docs).\nNote that in development a /mock path has been added. This allows the Dash web app to use all the mock endpoints instead of the live endpoints.\nThe environment variables are required, and the application will fail if any of these are missing when it attempts to run.\n\n\n7. Running The Servers Manually When Developing\nThe web server when in the hyui-web shell:\nenv $(grep -v '^#' .env | xargs) python -m web.app\nThe API server when in the hyui-api shell:\nenv $(grep -v '^#' .env | xargs) uvicorn api.main:app --reload --port 8092\nEnsure you replace .env with the correct path to your development .env file. The command env $(grep -v '^#' .env | xargs) loads the contents of the .env into the environment just for the current execution. The grep -v '^#' part excludes lines beginning with a # in the env file.\nThe --port specified in the hyui-api command should match the port in the WEB_API_URL variable of your .environment.\nYou can run both these commands from within vscode or pycharm - see instructions here."
  },
  {
    "objectID": "developer/setup.html#development-within-the-nhs",
    "href": "developer/setup.html#development-within-the-nhs",
    "title": "Setting up development for HyUi",
    "section": "Development within the NHS",
    "text": "Development within the NHS\nFurther information on developing within the NHS found on the Deployment page."
  },
  {
    "objectID": "developer/documentation.html",
    "href": "developer/documentation.html",
    "title": "HYLODE",
    "section": "",
    "text": "Writing documentation\nSet up for editing and building documentation All based around Quarto Documentation in ./docs\n\ninstalled basic python and jupyter extensions necessary including those for jupyter\ninstalled quarto\ninstalled the VSCode extension\n\nWorkflow\ncd docs\nquarto preview\nDonâ€™t forget that changes to config (e.g.Â _quarto.yml may need you to rerun quarto render to ensure the whole site is correctly rebuilt)\nWhen you push to either dev or main branches, then a GitHub Action (see .github/workflows/publish.yml) will be triggered that should publish the current documentation to https://hylode.github.io/HyUi/."
  },
  {
    "objectID": "developer/demo.html",
    "href": "developer/demo.html",
    "title": "Backend to frontend demonstration vignette",
    "section": "",
    "text": "Create a â€˜back endâ€™ endpoint using FastAPI\nWrite a simple test that checks that route exists\nCreate a â€˜front endâ€™ endpoint using Plotly Dash\nWrite a simple integration test that checks that the page displays\n\n\n\n\nCreate a new â€˜python packageâ€™ (directory containing an __init__.py) file under api/src/api/.\nCreate a router.py file within this package\nGenerate some fake/mock data ideally by hand. You might want to run the SQL script youâ€™re planning to use, then copy the column headings into an Excel file, and then create a couple of rows of fake data that you could/should save as a Python List of dictionaries or similar. You need to make this data available to your â€˜mockâ€™ endpoint.\ncreate a mock and a non-mock endpoint in router.py\nregister these endpoints in api/src/api/main.py\n\n# At the top of the file\nfrom api.demo.router import (\n    router as demo_router,\n    mock_router as mock_demo_router,\n)\n\n# further imports etc. ...\n\n# declare the app\napp = FastAPI(default_response_class=ORJSONResponse)\nmock_router = APIRouter(\n    prefix=\"/mock\",\n)\n\n# ...\napp.include_router(demo_router)\nmock_router.include_router(mock_demo_router)\n\n\n\n\n\n\n\nCreate a new â€˜python packageâ€™ (directory containing an __init__.py) file under web/src/web/pages (in this example named â€˜demoâ€™)\nCreate single page Plotly Dash application here\nThis will be automatically registered by the web/src/web/main.py module because of the use_pages=True argument passed to the Dash app\nThe page has 3 main components:\n\nA layout\nA data store\nA series of callbacks that fire as the user interacts with the data\n\nInspect at http://localhost:8201/demo/demo"
  },
  {
    "objectID": "developer/demo.html#ways-of-working",
    "href": "developer/demo.html#ways-of-working",
    "title": "Backend to frontend demonstration vignette",
    "section": "Ways of working",
    "text": "Ways of working\n\nPycharm\n\nsetup the FastAPI and Plotly Dash configurations so that you can see the changes as you work"
  },
  {
    "objectID": "developer/ide_setup.html",
    "href": "developer/ide_setup.html",
    "title": "IDE set-up",
    "section": "",
    "text": "Set up run configurations:\n\nPyCharm Run/Debug Configurations\nInstall the EnvFile plug-in so that the environment file can be read by your run configuration (see Stackoverflow Answer)\nSet OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES as an environment variable in your run configuration to avoid errors on recent macos installs\n\nExample configurations here - FastAPI - Plotly Dash\nIf you are using PyCharm or another IDE then ensure that the environment variables in .env are loaded. PyCharm has a plugin that can load these when unit testing with pytest or running the main application."
  },
  {
    "objectID": "developer/ide_setup.html#notes-on-setting-up-vscode-for-development",
    "href": "developer/ide_setup.html#notes-on-setting-up-vscode-for-development",
    "title": "IDE set-up",
    "section": "Notes on setting up VSCode for development",
    "text": "Notes on setting up VSCode for development\nBecause there are separate Python packages with separate interpreters you will need a â€˜multirootâ€™ workspace. I found this note from StackOverFlow helpful.\nOne small problem with sharing a VS Code configuration is that it seems to require absolute paths from the settings.json and launch.json files. With that caveat in mind, then here are some examples below.\n\nGit clone the project as per usual\nOpen the project in VSCode and save as a workspace\nEdit the root (HyUI) .vscode/settings.json file to exclude the folders holding the nested packages\n\n{\n    \"files.exclude\": {\n        \"**/.git\": true,\n        \"**/.svn\": true,\n        \"**/.hg\": true,\n        \"**/CVS\": true,\n        \"**/.DS_Store\": true,\n        \"**/Thumbs.db\": true,\n        // exclude nested modules/packages below\n        \"api\": true,\n        \"web\": true,\n        \"initialise\": true,\n        \"jupyter\": true,\n    }\n}\n\nAdd these folders back in via Workspaces >> Add folder to workspace â€¦ command (nested under the File menu). In my case, this means that my â€˜workspaceâ€™ file, now looks like this (where I have used the name property to help me visually navigate my workspace).\n\n{\n    \"folders\": [\n        {\n            \"path\": \".\",\n            \"name\": \"HyUi Project\"\n        },\n        {\n            \"path\": \"api\",\n            \"name\": \"HyUi FastAPI\"\n        },\n        {\n            \"path\": \"web\",\n            \"name\": \"HyUi Plotly Dash\"\n        }\n    ]\n}\nAnd my sidebar in VS Code (with the web labelled as â€˜HyUI Plotly Dashâ€™)\n\n\nThen prepare nested .vscode/settings.json and .vscode/launch.json files etc (also visible in the screenshot above). For example, for the web module, you need to specify the interpreter, and a debug configuration (in launch.json).\n\nBeware the absolute paths below â˜¹. You will need to change these unless your name is also â€˜steveâ€™.\nsettings.json:\n{\n    \"python.defaultInterpreterPath\": \"${userHome}/.pyenv/hyui-web/bin/python3\",\n    \"python.envFile\": \"${workspaceFolder}/.env\",\n    \"python.testing.pytestArgs\": [\n        \"src/web/\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true\n}\nlaunch.json under web:\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"HyUi: Plotly Dash\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"web.app\",\n            \"args\": [\n                \"--reload\"\n            ],\n            \"jinja\": true,\n            \"justMyCode\": true,\n            \"python\": \"/users/steve/.pyenv/versions/hyui-web/bin/python3\",\n            \"cwd\": \"${workspaceFolder}/src\",\n            \"envFile\": \"${workspaceFolder}/../.env\",\n            \"env\": {\n                \"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\": \"YES\",\n                \"PYTHONUNBUFFERED\": \"1\",\n        }\n        }\n    ]\n}\nlaunch.json under api:\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"hyui-api\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"uvicorn\",\n            \"args\": [\n                \"api.main:app\",\n                \"--reload\",\n                \"--port\",\n                \"8092\",\n            ],\n            \"jinja\": true,\n            \"justMyCode\": true,\n            \"python\": \"/users/steve/.pyenv/versions/hyui-api/bin/python3\",\n            \"cwd\": \"${workspaceFolder}/src\",\n            \"envFile\": \"${workspaceFolder}/../.env\",\n            \"env\": {\n                \"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\": \"YES\",\n                \"PYTHONUNBUFFERED\": \"1\",\n            }\n        }\n    ]\n}\n\nTroubleshooting\n\nFollow all the setup instructions and check you can get everything running locally outside vscode first!\nCheck that the locations of python and your .env file are correct in your launch.json and settings.json file. You may need to write absolute paths in settings.json (even if launch.json uses the {workSpaceFolder} variables appropriately). For example, \"python.envFile\": \"~/Github/HyUi/.env\")."
  },
  {
    "objectID": "developer/deployment.html",
    "href": "developer/deployment.html",
    "title": "HYLODE",
    "section": "",
    "text": "Clone the repository on an NHS server git clone https://github.com/HYLODE/HyUi.git.\nConfigure your .env variables using .env.example as a template.\n\n\n\n\nIf setting up a â€˜sharedâ€™ deployment (a named dev deployment for example), ensure that permissions are set correctly on the directory to allow git pulls and docker builds by multiple users. On the deployment GAEs users are assigned to the docker group, so permissions can be changed as follows\ncd /path/to/git/repository\n\n# Give group access to any new local modifications to the repo\ngit init --shared=group\n\n# Give the group access to all existing items in the repository\nchown -R :docker \"$PWD\"\nchmod -R g+swX \"$PWD\"\nThis is expected to work optimally with a freshly cloned git repository. If working with an existing local repository containing multiple changes, docker builds or git pull/pushes may throw permissions errors as another user may have set restrictive permissions on a file crucial for either process. In many cases it is simplest to clone the repository from scratch and perform the steps above.\nAs an alternative if the above is not working, you should try\nchgrp -R docker /path/to/repository\nchmod -R g+rws /path/to/repository\nsetfacl -R -m d:g::rwX /path/to/repository\n\n\nWe use Varnish as a HTTP cache to speed up some of the slower API calls made by the front end. The default behaviour we follow is to skip the cache and manually identify endpoints which would benefit from caching.\nThe cache is brought up with other services and is represented in compose.yml, which uses docker/cache/Dockerfile. The only change required to utilise caching is within the base .env file. The WEB_API_URL environment variable should be set to http://cache:8000 for deployment on the hospital network or http://cache:8000/mock for local development with docker compose.\nCache configuration is set in cache/default.vcl and contains very minimal additions with respect to a basic varnish configuration.\nEndpoints can be added to the cache by altering the Cache-Control response header. An example is provided in api/src/api/beds/router.py under /mock/baserow/beds/, where the cache lifetime is set to 300 seconds with response.headers[\"Cache-Control\"] = \"public, max-age=300\".\nThe first request made to an uncached API endpoint will not see any speed up - subsequent requests within the time to live (TTL) of an object will use the cached object and will see a speedier response.\nAdditional configuration is provided for endpoints that absolutely must return live data from the API. In this case, there are two options for skipping the cache.\n\nadding X-Varnish-Nuke:1 to the request header will clear the cache for the current endpoint and force a live query of the API\nAdding Cache-Control: no-cache to the request header will instruct Varnish to skip the cache for the requested endpoint\n\nBy relying on the setting of cached object TTL via response headers and the skipping (or clearing of) the cache with an additional header, the hope is that developers should not need to interact with default.vcl and all changes can be made via web standards.\nThe web service in compose.yml contains a healthcheck element that runs the docker/web/refresh_cache.sh script at a regular specified time interval. Via this script, endpoints can be called via curl with the X-Varnish-Nuke:1 header to force regular updating of the cached endpoint. This is particularly useful for slow-to-load endpoints that should be available on demand to users.\n\n\nBy default, endpoints are not cached. This is enforced with response headers set via middleware in api/src/api/main.py added as a decorator to the app and mock_router FastAPI instances.. This default is overriden for specified endpoints by altering the response headers as described previously.\nTTLs for individual endpoints can be altered by adding Cache-Control: max-age=<ttl>, where <ttl> represents the desired TTL in seconds.\n\n\n\n\n\nIn the base directory (containing compose.yml)\ndocker compose --project-name [project name] up --build\nEnsure that you use a unique name for your [project name]. The production service should be named hyui-prod.\n\n\n\nThis should be done the first time you stand up the services. It populates the Baserow database with the tables and static data required to run the rest of the HyUi services. You will also need to do this when you update your configuration (loading) data for Baserow. That is, you should consider Baserow as a temporary scratch pad, and mapping changes etc. will live in code (see initialise/src/initialise/bed_defaults.json). Before you â€˜redeployâ€™, you must manually delete the tables from Baserow.\nYou may also want to register the license for the self hosted version. To do this, check that the â€˜instance idâ€™ available on the admin pages is copied from the self-hosted version to the web / hosted version; then regenerate the license key on the hosted version; then copy that license key back to the self-hosted; then add the user to the â€˜seatâ€™ created by that registered version. This enables â€˜export to JSONâ€™ functionality etc.\n./bin/initialise.sh --operation baserow\n\n\nTo be used when updating an existing instance of baserow adjustments to default tables\n\nloads fresh versions of the default tables including\n\ndepartments\n\n\n./bin/initialise.sh --operation recreate_defaults\n\n\n\n\ndocker compose --project-name [project name] down\n\n\n\n\n\nSometimes it can be difficult to debug your staging services when they are deployed via docker.\nYou can log into the hyui-web or hyui-api services from a terminal within the HyUi directory using the following:\ndocker compose --project-name [project name] exec web bash\ndocker compose --project-name [project name] exec api bash\nYou can then change the code using vim or open a python console and debug from there.\nRemember you will then need to manually make the same changes outside the docker bash terminal!\nIt may be useful to edit the docker/api/Dockerfile or docker/web/Dockerfile and add a --reload flag to the ENTRYPOINT so that code changes from within the container are reloaded automatically. ENTRYPOINT [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--reload\"] instead of ENTRYPOINT [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--reload\"].\nIf you prefer not to use vim then Visual Studio can SSH into the running containers somehow."
  },
  {
    "objectID": "notes/provisioning.html",
    "href": "notes/provisioning.html",
    "title": "HYLODE",
    "section": "",
    "text": "Data access at UCLH must overcome two barriers: permissions and provisioning. Permissions (research ethics etc.) are arduous but clear, and (rightly) the responsibility of the researcher. But at UCLH, we are stuck on the provisioning step. This is because the researcher (the data consumer) and the hospital (the provider) are separate entities. The researcher is supposed to provide a precise data specification. The hospital is supposed to extract and anonymise.\nThis is possible for small studies, but slow, expensive and eventually impossible for Machine Learning for Health (ML4H). Here data requirements are established iteratively and cannot be specified a priori. Typically a researcher must first explore the full data set, and then works alongside domain experts (i.e.Â the clinical team) to build features from the data that serve the modelling task. Full anonymisation is a specialist skill not widely available among researchers or hospital data teams. Most extracts are de-identified not anonymised, but even this blocks future attempts to test models in the clinical environment since model predictions cannot be linked back to patients.1\nData provisioning currently divides these tasks between researchers and the hospital, and runs them sequentially: 1. Requirement specification [researcher] 2. Extraction and anonymisation [hospital] 3. Model building [researcher] 4. Model deployment [hospital]\nNeither side has the resources to support this properly. At present, there are only two full time data engineers in the CRIU but even a large team could not compete with the domain knowledge and efficiency of a researcher-clinician dyad.\nWe need instead a model that sees the provider/consumer roles merged. A sketch of the arrangement follows: 1. The CRIU would maintain a pseudonymised view of the commonest data sources in the trust (Epic reporting data warehouses, EMAP etc.). Access would be layered so that higher risk items such as free text required separate permissions. [Safe data] 2. Researchers would be required to follow a similar training to existing substantive NHS staff working with patient data (e.g.Â those in information services) or even held to a higher standard (e.g.Â as per the ONS approved researcher scheme). They would work under the supervision and line management of CRIU staff with an (honorary) contractual relationship to the hospital.[Safe People] 3. Data extraction and analysis would occur in the hospital but be performed by the research team using the pseudonymised data. [Safe setting] 4. The CRIU would maintain a two layer platform (as per Nel Swanepoelâ€™s FlowEHR architecture) that allows research models trained on pseudonymised data to be deployed against identifiable data. This would enable ML4H algorithms to be tested in the clinical environment without the researchers needing access to identifiable data.\nThis proposal follows both the Five Safes principles, and is similar to Precedent 3 under the guidance issued by the Confidential Advisory Group: â€œaccessing data on-site to extract anonymised dataâ€. Under the Five Safes, we are offsetting a reduction in the safety of the data (de-identified rather than anonymised) by increasing the safety of the people, the setting and the outputs. Precedent 3 recognises that data preparation by researchers is legitimate when it is not practicable for the direct care team to fulfil that role. We have already taken this approach to the BRC PPI panel on the use of data, and explicitly discussed this trade-off between the â€˜Five Safesâ€™. The technical work to pseudonymise existing health data is manageable, and we have a working implementation of the development/deployment platform. The next step had been to take a version of this proposal to the trust information governance team for comment."
  },
  {
    "objectID": "user/user.html",
    "href": "user/user.html",
    "title": "User guide for HYLODE",
    "section": "",
    "text": "Your first port of call for all questions HySys should be the top-level repository documentation. Some tips and tricks for particularly common patterns are given here.\n\n\nDeployment instructions for a fresh installation of HySys are provided in the readme linked to above. To redeploy an already existing deployment on the NHS network:\n\nNavigate to the directory containing the checked out repository. Locations are provided here for the staging_red and prod deployments.\nNavigate to the the hylode/ directory (containing docker-compose.yml)\nFrom within a tmux or screen session, execute the bin/up.sh script to deploy most of the back-end HySys services\n\nyou can execute this by running ./bin/up.sh on the command line\nitâ€™s recommended to run this command inside a tmux or screen session\n\n\nThis will redeploy all non-interactive services - everything but the HyMind Lab and Field.\n\n\nTo deploy the HyMind Lab and HyMind Field services, execute ./bin/interactive-up.sh. Again, itâ€™s recommended that you do this within a screen or tmux session.\n\n\n\n\nTo take down all running services - including interactive services described above:\n\nNavigate to the directory containing the checked out repository. Locations are provided here for the staging_red and prod deployments.\nNavigate to the the hylode/ directory (containing docker-compose.yml)\nRun ./bin/down.sh from the command line\n\nOptionally provide the --remove-orphans flag to remove hanging containers and networks. This will not remove any associated docker volumes, so is safe from a data-loss perspective.\n\n\n\n\nCurrent and past model predictions arising as a result of the HySys pipeline are written to corresponding tables within the hymind schema within the hylode database found in the postgres service of each environment.\nThe latest predictions are available via the HyMind API REST endpoint. The HySys documentation details the URL of the documentation of this API - look for HYMIND_API_WEB_PORT.\n\n\n\n\n\nthe application is only accessible from within the UCLH network\nusernames and passwords are available after training from the development team"
  },
  {
    "objectID": "vignettes/icu-tap-aggregate-probs.html",
    "href": "vignettes/icu-tap-aggregate-probs.html",
    "title": "HYLODE",
    "section": "",
    "text": "import json\nfrom datetime import date\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nfrom scipy.stats import binom\n\ndata = json.dumps(\n    {\n        \"horizon_dt\": date.today().strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\n        \"department\": \"gwb\",\n    }\n)\n\n\nelective_preds = requests.post(\"http://uclvlddpragae08:5219/predict/\", data).json()\nnonelective_preds = requests.post(\"http://uclvlddpragae08:5230/predict/\", data).json()\n\n\nelective = []\nfor bed_num in elective_preds[\"data\"]:\n    elective.append(bed_num[\"probability\"])\nprint(\n    \"If the number of elective patients on list is \" + str(len(elective) - 1),\n    \"the ML model will return a set of predictions from 0 beds, needed, up to that maximum, so our first array has \"\n    + str(len(elective))\n    + \" values, starting with zero\",\n)\nposs_beds = list(range(0, len(elective)))\nprint(poss_beds)\nfig, ax = plt.subplots(1, 1)\nplt.scatter(poss_beds, elective)\nplt.title(\"Distribution of elective probabilities\")\nplt.show()\n\n\nnonelective = []\nfor bed_num in nonelective_preds[\"data\"]:\n    nonelective.append(bed_num[\"probability\"])\nprint(\n    \"The ML model will return predictions for a set of beds needed for nonelective patients that might arrive ranging from 0 to  \"\n    + str(len(nonelective) - 1),\n    \" so our second array has \" + str(len(nonelective)) + \" values, starting with zero\",\n)\nposs_beds = list(range(0, len(nonelective)))\nprint(poss_beds)\n\n\nfig, ax = plt.subplots(1, 1)\nplt.scatter(poss_beds, nonelective)\nplt.title(\"Distribution of nonelective probabilities\")\nplt.show()\n\n\nnum_on_unit = 18\nprob_still_here = 0.8\nonunit = binom.pmf(list(range(0, num_on_unit + 1)), num_on_unit, prob_still_here)\nprint(\n    \"Let's say there are \"\n    + str(num_on_unit)\n    + \" already on the unit each with a probability of \"\n    + str(prob_still_here)\n    + \" of still being here in 24 hours time, so our third array has \"\n    + str(len(onunit))\n    + \" values, starting with zero\"\n)\nposs_beds = list(range(0, len(onunit)))\nprint(poss_beds)\n\n\nfig, ax = plt.subplots(1, 1)\nplt.scatter(poss_beds, onunit)\nplt.title(\"Distribution of probabilities for pats on unit\")\nplt.show()\n\n\nprint(\n    \"The total possible number of admissions is the sum of the maximum possible from each of these separate distributions, which is \"\n    + str(num_on_unit + len(elective) - 1 + len(nonelective) - 1),\n    \"so the length of our aggregated predictions is one more than this number, as it is starting at zero\",\n)\ntot_beds_array = list(range(0, len(onunit) + len(elective) - 1 + len(nonelective) - 1))\nprint(tot_beds_array)\n\n\nprint(\n    \"To get a probability distribution over all these, we simply convolve them together like this: \"\n)\naggregated = np.convolve(onunit, np.convolve(elective, nonelective))\n\n\nfig, ax = plt.subplots(1, 1)\nplt.scatter(tot_beds_array, aggregated)\nplt.title(\"Distribution of aggregated probabilities\")\nplt.show()"
  },
  {
    "objectID": "vignettes/beds_v3UCLH.html",
    "href": "vignettes/beds_v3UCLH.html",
    "title": "HYLODE",
    "section": "",
    "text": "2022-07-03\nworking notes on merging caboodle info on beds with EMAP\naim is to build a reliable view of bed census\nprogrammatically returns a list of locatoins with Caboodle detail\nbut no testing/quality control\nworks in conjunction with add_caboodle2emap_beds.py\nuse to populate base bed definitions for the tower flow"
  },
  {
    "objectID": "vignettes/beds_v3UCLH.html#set-up-incl-database-connections",
    "href": "vignettes/beds_v3UCLH.html#set-up-incl-database-connections",
    "title": "HYLODE",
    "section": "Set-up incl database connections",
    "text": "Set-up incl database connections\n\nimport os\nimport urllib\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\n\n\ndef emap_db() -> Engine:\n    url = \"postgresql+psycopg2://{}:{}@{}:{}/{}\".format(\n        os.getenv(\"EMAP_DB_USER\"),\n        os.getenv(\"EMAP_DB_PASSWORD\"),\n        os.getenv(\"EMAP_DB_HOST\"),\n        os.getenv(\"EMAP_DB_PORT\"),\n        os.getenv(\"EMAP_DB_NAME\"),\n    )\n    engine = create_engine(\n        url, pool_size=4, max_overflow=6, connect_args={\"connect_timeout\": 120}\n    )\n    return engine\n\n\ndef caboodle_db() -> Engine:\n    db_host = os.getenv(\"CABOODLE_DB_HOST\")\n    db_user = os.getenv(\"CABOODLE_DB_USER\")\n    db_password = os.getenv(\"CABOODLE_DB_PASSWORD\")\n    db_port = os.getenv(\"CABOODLE_DB_PORT\")\n    db_name = os.getenv(\"CABOODLE_DB_NAME\")\n    connection_str = f\"mssql+pyodbc://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server\"\n    engine = create_engine(connection_str)\n    return engine\n\n\nemap_engine = emap_db()\ncaboodle_engine = caboodle_db()"
  },
  {
    "objectID": "vignettes/beds_v3UCLH.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "href": "vignettes/beds_v3UCLH.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "title": "HYLODE",
    "section": "Reliable way of joining Caboodle and EMAP bed level data",
    "text": "Reliable way of joining Caboodle and EMAP bed level data\nFirst load EMAP location table into memory\nDo this with no modifications / transformations other than the join\n\nq = \"\"\"\nSELECT \n    lo.location_id, lo.location_string, lo.department_id, lo.room_id, lo.bed_id, dept.name department, dept.speciality, room.name room\nFROM star.location lo\nLEFT JOIN star.department dept ON lo.department_id = dept.department_id\nLEFT JOIN star.room ON lo.room_id = room.room_id\n\"\"\"\ndfe = pd.read_sql_query(q, emap_engine)\ndfe.head()\n\nAnd load Caboodle DepartmentDim into memory\n\nq = \"\"\"\nSELECT \n DepartmentKey\n,BedEpicId\n,Name\n,DepartmentName\n,RoomName\n,BedName\n,IsBed\n,BedInCensus\n,IsDepartment\n,IsRoom\n,IsCareArea\n,DepartmentExternalName\n,DepartmentSpecialty\n,DepartmentType\n,DepartmentServiceGrouper\n,DepartmentLevelOfCareGrouper\n,LocationName\n,ParentLocationName\n,_CreationInstant\n,_LastUpdatedInstant\nFROM dbo.DepartmentDim\n\"\"\"\ndfc = pd.read_sql_query(q, caboodle_engine)\ndfc.head()\n\nNow join these two tables - donâ€™t attempt joins where there is no room/bed level data (b/c theyâ€™re not physical locations - you need a multi-key join - EMAP â€˜nameâ€™ (department.name) joins on to Caboodle â€˜DepartmentNameâ€™ - EMAP â€˜bedâ€™ (derived by splitting location.location_string) joins on to Caboodle â€˜Nameâ€™ - drop â€˜waitâ€™ beds since these duplicate and block a one-to-one merge - try to be rigorous in pd.merge - indicator=True to allow inspection post merge - validate='one_to_one' to throw an error if duplicates found\nNote - sometimes (in Caboodle) DepartmentName and Name are duplicated so pick the most recently â€˜createdâ€™\nusing â€¦\nNow load external code\n\nimport importlib\n\nimport add_caboodle2emap_beds\n\nimportlib.reload(add_caboodle2emap_beds)\nfrom add_caboodle2emap_beds import bed_merge\n\n\ndepartments = [\n    # Built from Tower Report 14 Jun 2022\n    # NAME                         # n emap locations\n    \"UCH T01 ACUTE MEDICAL\",  # 86\n    \"UCH T01 ENHANCED CARE\",  # 20\n    \"UCH T03 INTENSIVE CARE\",  # 37\n    \"UCH T06 HEAD (T06H)\",  # 27\n    \"UCH T06 CENTRAL (T06C)\",  # 25\n    \"UCH T06 SOUTH PACU\",  # 22\n    \"UCH T06 GYNAE (T06G)\",  # 18\n    \"UCH T07 NORTH (T07N)\",  # 45\n    \"UCH T07 CV SURGE\",  # 37\n    \"UCH T07 SOUTH\",  # 33\n    \"UCH T07 SOUTH (T07S)\",  # 23\n    \"UCH T07 HDRU\",  # 20\n    \"UCH T08 NORTH (T08N)\",  # 28\n    \"UCH T08 SOUTH (T08S)\",  # 25\n    \"UCH T08S ARCU\",  #  6\n    \"UCH T09 SOUTH (T09S)\",  # 34\n    \"UCH T09 NORTH (T09N)\",  # 32\n    \"UCH T09 CENTRAL (T09C)\",  # 25\n    \"UCH T10 SOUTH (T10S)\",  # 34\n    \"UCH T10 NORTH (T10N)\",  # 32\n    \"UCH T10 MED (T10M)\",  # 16\n    \"UCH T11 SOUTH (T11S)\",  # 27\n    \"UCH T11 NORTH (T11N)\",  # 25\n    \"UCH T11 EAST (T11E)\",  # 16\n    \"UCH T11 NORTH (T11NO)\",  #  8\n    \"UCH T12 SOUTH (T12S)\",  # 32\n    \"UCH T12 NORTH (T12N)\",  # 23\n    \"UCH T13 SOUTH (T13S)\",  # 31\n    \"UCH T13 NORTH ONCOLOGY\",  # 26\n    \"UCH T13 NORTH (T13N)\",  # 26\n    \"UCH T14 NORTH TRAUMA\",  # 28\n    \"UCH T14 NORTH (T14N)\",  # 28\n    \"UCH T14 SOUTH ASU\",  # 22\n    \"UCH T14 SOUTH (T14S)\",  # 17\n    \"UCH T15 SOUTH DECANT\",  # 21\n    \"UCH T15 SOUTH (T15S)\",  # 21\n    \"UCH T15 NORTH (T15N)\",  # 16\n    \"UCH T15 NORTH DECANT\",  # 15\n    \"UCH T16 NORTH (T16N)\",  # 19\n    \"UCH T16 SOUTH (T16S)\",  # 18\n    \"UCH T16 SOUTH WINTER\",  # 17\n    \"GWB L01 ELECTIVE SURG\",  # 37\n    \"GWB L01 CRITICAL CARE\",  # 12\n    \"GWB L02 NORTH (L02N)\",  # 19\n    \"GWB L02 EAST (L02E)\",  # 19\n    \"GWB L03 NORTH (L03N)\",  # 19\n    \"GWB L03 EAST (L03E)\",  # 19\n    \"GWB L04 NORTH (L04N)\",  # 20\n    \"GWB L04 EAST (L04E)\",  # 17\n    \"WMS W04 WARD\",  # 28\n    \"WMS W03 WARD\",  # 27\n    \"WMS W02 SHORT STAY\",  # 20\n    \"WMS W01 CRITICAL CARE\",  # 11\n]\n\n\ndepartments = [\n    # Built from Tower Report 14 Jun 2022\n    # NAME                         # n emap locations\n    \"UCH T01 ACUTE MEDICAL\",  # 86\n    \"UCH T01 ENHANCED CARE\",  # 20\n    \"UCH T03 INTENSIVE CARE\",  # 37\n    \"UCH T06 HEAD (T06H)\",  # 27\n    \"UCH T06 CENTRAL (T06C)\",  # 25\n    \"UCH T06 SOUTH PACU\",  # 22\n    \"UCH T06 GYNAE (T06G)\",  # 18\n    \"UCH T07 NORTH (T07N)\",  # 45\n    \"UCH T07 CV SURGE\",  # 37\n    \"UCH T07 SOUTH\",  # 33\n    \"UCH T07 SOUTH (T07S)\",  # 23\n    \"UCH T07 HDRU\",  # 20\n    \"UCH T08 NORTH (T08N)\",  # 28\n    \"UCH T08 SOUTH (T08S)\",  # 25\n    \"UCH T08S ARCU\",  #  6\n    \"UCH T09 SOUTH (T09S)\",  # 34\n    \"UCH T09 NORTH (T09N)\",  # 32\n    \"UCH T09 CENTRAL (T09C)\",  # 25\n    \"UCH T10 SOUTH (T10S)\",  # 34\n    \"UCH T10 NORTH (T10N)\",  # 32\n    \"UCH T10 MED (T10M)\",  # 16\n    \"UCH T11 SOUTH (T11S)\",  # 27\n    \"UCH T11 NORTH (T11N)\",  # 25\n    \"UCH T11 EAST (T11E)\",  # 16\n    \"UCH T11 NORTH (T11NO)\",  #  8\n    \"UCH T12 SOUTH (T12S)\",  # 32\n    \"UCH T12 NORTH (T12N)\",  # 23\n    \"UCH T13 SOUTH (T13S)\",  # 31\n    \"UCH T13 NORTH ONCOLOGY\",  # 26\n    \"UCH T13 NORTH (T13N)\",  # 26\n    \"UCH T14 NORTH TRAUMA\",  # 28\n    \"UCH T14 NORTH (T14N)\",  # 28\n    \"UCH T14 SOUTH ASU\",  # 22\n    \"UCH T14 SOUTH (T14S)\",  # 17\n    \"UCH T15 SOUTH DECANT\",  # 21\n    \"UCH T15 SOUTH (T15S)\",  # 21\n    \"UCH T15 NORTH (T15N)\",  # 16\n    \"UCH T15 NORTH DECANT\",  # 15\n    \"UCH T16 NORTH (T16N)\",  # 19\n    \"UCH T16 SOUTH (T16S)\",  # 18\n    \"UCH T16 SOUTH WINTER\",  # 17\n    \"GWB L01 ELECTIVE SURG\",  # 37\n    \"GWB L01 CRITICAL CARE\",  # 12\n    \"GWB L02 NORTH (L02N)\",  # 19\n    \"GWB L02 EAST (L02E)\",  # 19\n    \"GWB L03 NORTH (L03N)\",  # 19\n    \"GWB L03 EAST (L03E)\",  # 19\n    \"GWB L04 NORTH (L04N)\",  # 20\n    \"GWB L04 EAST (L04E)\",  # 17\n    \"WMS W04 WARD\",  # 28\n    \"WMS W03 WARD\",  # 27\n    \"WMS W02 SHORT STAY\",  # 20\n    \"WMS W01 CRITICAL CARE\",  # 11\n]\n\n\ndfm = bed_merge(df_emap=dfe, df_caboodle=dfc, departments=departments)\ndfm.head()\n\n\ndfm._merge.value_counts()\n\n\ndfm.to_csv(\"beds.tsv\", sep=\"\\t\", index_label=\"local_id\")"
  },
  {
    "objectID": "vignettes/icu-tap-elective-predictions.html",
    "href": "vignettes/icu-tap-elective-predictions.html",
    "title": "HYLODE",
    "section": "",
    "text": "This notebook shows how to use the models I have saved to ML flow, for the purposes of predicting how many elective patients will arrive in the ICU in the 24 hours following 9.15 on a given day.\nOne of three locations can be requested: tower, gwb and wms (all lower case). These have different trained models. You need to retrieve the relevant model from ML flow for the location requested. You would do this be setting model_name and model_version to the saved values (as an example for the tower: MODEL__TAP_ELECTIVE_TOWER__NAME, MODEL__TAP_ELECTIVE_TOWER__VERSION need to be set; these should eventually be saved as constants in global settings)\nLogic: - Find out how many patients are on the surgical list for a given day - Retrieve a ML model which uses simple date parameters (but could one day be more sophisticated) to determine probability of ICU admission for any patient (not taking patient level characteristics into account) based on day of week - Use this probability to generate a binomial probability distribution over the number of beds needed in ICU for those patients\nNOTES - we decided not to make predictions for weekends, as almost all weekend days have no elective patients - we use a lens here which does not really serve any purpose, other than to one-hot encode the day of week. However, in future, other covariates (eg the number of patients in the hospital currently) could be added to the model, and the lens is already in place to add scaling functions\n\n\nThe input data to the models takes the form of of one-row dataframe with the following columns; [â€˜model_functionâ€™, â€˜dateâ€™, â€˜icu_countsâ€™, â€˜noticu_countsâ€™, â€˜wkdayâ€™, â€˜Nâ€™] - model_function - set this to â€˜binomâ€™ - date - use pd.to_datetime to set format eg pd.to_datetime(â€˜2022-08-08â€™) - icu_counts - set this field to zero [this is an artefact of the lens method] - noticu_counts - ditto - wkday - an integer for the day of the week, where Monday is 0 - N - number of patients in the elective list (retrieved from Caboodle)\n\n\n\nThe called function SKProbabilityPredictorStats() will return an error if: - you request a weekend day - there are no patients on the elective list\n\n\n\n\nimport pkg_resources\n\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([f\"{i.key}=={i.version}\" for i in installed_packages])\n\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\n\nimport os\nimport pickle\nimport tempfile\nfrom pathlib import Path\n\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\n\nimport urllib\n\nfrom hylib import settings\nfrom hylib.dt import LONDON_TZ\nfrom hymind.predict.base import BaseLensPredictor\nfrom patsy import dmatrices\nfrom scipy.stats import binom\nfrom sqlalchemy import create_engine\n\n\n\n\n\nconn_str = \"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={},{};DATABASE={};UID={};PWD={}\".format(\n    settings.CABOODLE_DB_HOST,\n    settings.CABOODLE_DB_PORT,\n    settings.CABOODLE_DB_NAME,\n    settings.CABOODLE_DB_USER,\n    settings.CABOODLE_DB_PASSWORD,\n)\ncaboodle_db = create_engine(\n    f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(conn_str)}\"\n)\n\n\n\n\n\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n\nmlflow_var = os.getenv(\"HYMIND_REPO_TRACKING_URI\")\nmlflow.set_tracking_uri(mlflow_var)\n\nclient = MlflowClient()\n\n\nMODEL__TAP_ELECTIVE_TOWER__NAME = \"tap_elective_tower\"\nMODEL__TAP_ELECTIVE_TOWER__VERSION = 5\nMODEL__TAP_ELECTIVE_GWB__NAME = \"tap_elective_gwb\"\nMODEL__TAP_ELECTIVE_GWB__VERSION = 5\nMODEL__TAP_ELECTIVE_WMS__NAME = \"tap_elective_wms\"\nMODEL__TAP_ELECTIVE_WMS__VERSION = 5\n\n\ndef get_model_details(location):\n\n    if location == \"tower\":\n        model_name, model_version = (\n            MODEL__TAP_ELECTIVE_TOWER__NAME,\n            MODEL__TAP_ELECTIVE_TOWER__VERSION,\n        )\n    elif location == \"gwb\":\n        model_name, model_version = (\n            MODEL__TAP_ELECTIVE_GWB__NAME,\n            MODEL__TAP_ELECTIVE_GWB__VERSION,\n        )\n    else:\n        model_name, model_version = (\n            MODEL__TAP_ELECTIVE_WMS__NAME,\n            MODEL__TAP_ELECTIVE_WMS__VERSION,\n        )\n    return model_name, model_version\n\n\n\n\n\nclass SKProbabilityPredictorStats(BaseLensPredictor):\n    def __init__(self, model_name: str, model_version: int) -> None:\n        super().__init__(model_name, model_version)\n        self.model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n        self.expr = self._load_expr(self.model_info.run_id)\n        self.lens = self._load_lens(self.model_info.run_id)\n        self.input_df = self._is_weekday(input_df)\n        self.input_df = self._elective_list_gt0(input_df)\n\n    def _is_weekday(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if not input_df.iloc[0, 4] in list(range(0, 5)):\n                raise ValueError(\"Date requested is not a weekday\")\n            return input_df\n\n    def _elective_list_gt0(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if input_df.iloc[0, 5] == 0:\n                raise ValueError(\"There are no patients on the elective list\")\n            return input_df\n\n    @staticmethod\n    def _load_expr(run_id: str):\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp_dir = Path(tmp)\n\n            client.download_artifacts(run_id, \"expr\", tmp_dir)\n\n            expr_path = next((tmp_dir / \"expr\").rglob(\"*.txt\"))\n            with open(expr_path, \"rb\") as f:\n                expr = f.read()\n            expr = str(expr, \"utf-8\")\n\n            return expr\n\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n\n        X_df = self.lens.transform(input_df)\n        X_df__, X_df = dmatrices(self.expr, X_df, return_type=\"dataframe\")\n\n        predictions_set_df = self.model.get_prediction(X_df)\n        p = predictions_set_df.summary_frame().iloc[0, 0]\n\n        if input_df.iloc[0, 0] == \"binom\":\n\n            N = input_df.iloc[0, 5]\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": binom.pmf(list(range(0, N + 1)), N, p),\n                }\n            )\n\n        else:\n\n            N = 11\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": poisson.pmf(list(range(0, N + 1)), p),\n                }\n            )\n\n        predictions_df[\"predict_dt\"] = datetime.now(LONDON_TZ)\n        predictions_df[\"model_name\"] = self.model_name\n        predictions_df[\"model_version\"] = self.model_version\n        predictions_df[\"run_id\"] = self.model_info.run_id\n\n        return predictions_df\n\n\n\n\n\ndef get_elective_cases(date, location):\n\n    if location == \"tower\":\n\n        department1 = \"UCH P03 THEATRE SUITE\"\n        department2 = \"UCH T02 DAY SURG THR\"\n\n    elif location == \"wms\":\n\n        department1 = \"WMS W01 THEATRE SUITE\"\n        department2 = \"WMS W01 THEATRE SUITE\"\n\n    elif location == \"gwb\":\n\n        department1 = \"GWB B-1 THEATRE SUITE\"\n        department2 = \"GWB B-1 THEATRE SUITE\"\n\n    data = pd.read_sql(\n        \"\"\"\n        SELECT COUNT (DISTINCT scf.[PatientKey]) \n  \n      FROM [CABOODLE_REPORT].[dbo].[SurgicalCaseFact] scf \n      LEFT JOIN [CABOODLE_REPORT].[dbo].[WaitingListEntryFact] wlef ON wlef.[SurgicalCaseKey] = scf.[SurgicalCaseKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[SurgicalCaseUclhFactX] scufx ON scf.[SurgicalCaseKey] = scufx.[SurgicalCaseKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[PatientDim] patd ON scf.[PatientDurableKey] = patd.[DurableKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[ProcedureDim] pd ON scf.[PrimaryProcedureKey] = pd.[ProcedureKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DepartmentDim] dd ON scf.[OperatingRoomKey] = dd.[DepartmentKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datewl ON wlef.[PlacedOnWaitingListDateKey] = datewl.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datedta ON wlef.[DecidedToAdmitDateKey] = datedta.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datesurg ON scf.[SurgeryDateKey] = datesurg.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datecasereq ON scf.[CaseRequestDateKey] = datecasereq.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[TimeOfDayDim] todcase ON scf.[CaseRequestTimeOfDayKey] = todcase.[TimeOfDayKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datecancel ON scufx.[CancelDateKey] = datecancel.[DateKey]\n      WHERE scf.[PatientDurableKey] > 1 AND scf.[PatientDurableKey] IS NOT NULL\n    --  AND wlef.[SurgicalService] != '*Unspecified' \n      AND scf.[PrimaryService] != 'Obstetrics' AND scf.[PrimaryService] != 'Neurosurgery' AND scf.[PrimaryService] != 'Paediatric Dental'\n      AND scf.[PatientInFacilityDateKey] < 0\n      AND dd.[DepartmentName] != 'NHNN THEATRE SUITE' AND dd.[DepartmentName] != 'RNTNE THEATRE SUITE' AND dd.[DepartmentName] != 'EGA E02 LABOUR WARD'\n      AND dd.[DepartmentName] != 'MCC H-1 THEATRE SUITE' AND dd.[DepartmentName] != 'UCH ANAESTHESIA DEPT'\n      --AND dd.[DepartmentName] != 'UCH P02 ENDOSCOPY'\n      AND patd.[AgeInYears] >= 18\n      AND (wlef.[IntendedManagement] IN ('*Unspecified', 'Inpatient', 'Inpatient Series', 'Night Admit Series') OR wlef.[IntendedManagement] IS NULL)\n      AND CONVERT(DATE, scufx.[PlannedOperationStartInstant]) = ?\n      AND ((scf.[Classification] = 'Elective') OR (scf.[Classification] = 'Expedited (within 2 weeks on elective list)'))\n      AND ((dd.[DepartmentName] = ?) OR (dd.[DepartmentName] = ?))\n      \n        \"\"\",\n        caboodle_db,\n        params=[date, department1, department2],\n    )\n\n    return data\n\n\n\n\n\n## Create input matrix for model, specifying a date to make a prediction for. The model will return a probability of admission to ICU\n\ndate = \"2022-08-05\"\nto_predict = pd.to_datetime(date)\nlocation = \"gwb\"\nmodel_name, model_version = get_model_details(location)\n\ninput_df = pd.DataFrame(\n    np.array([\"binom\", to_predict, 0, 0])[np.newaxis],\n    columns=[\"model_function\", \"date\", \"icu_counts\", \"noticu_counts\"],\n)\ninput_df.loc[:, \"wkday\"] = (\n    input_df.loc[:, \"date\"].apply(datetime.weekday).astype(\"object\")\n)\ninput_df[\"date\"] = input_df[\"date\"].values.astype(np.float)\ninput_df[\"N\"] = get_elective_cases(to_predict, location)\ninput_df.columns\n\n\npredictor = SKProbabilityPredictorStats(model_name, model_version)\npredictions_df = predictor.predict(input_df)\n\n\npredictions_df[\"probability\"].values\n\n\n\n\n\n# Retrieve the model and the saved lens\n\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\nprint(model.summary())\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n\n    client.download_artifacts(run_id, \"lens\", tmp_dir)\n\n    lens_path = next((tmp_dir / \"lens\").rglob(\"*.pkl\"))\n    with open(lens_path, \"rb\") as f:\n        lens = pickle.load(f)\n\nX_df = lens.transform(input_df)\nX_df__, X_df = dmatrices(expr, X_df, return_type=\"dataframe\")"
  },
  {
    "objectID": "vignettes/vignettes.html",
    "href": "vignettes/vignettes.html",
    "title": "Vignettes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nWorking notes on merging Caboodle and EMAP (NHNN)\n\n\n\n\n\n\nWorking notes on merging caboodle info on beds with EMAP (UCLH)\n\n\n\n\n\n\nICU aggregate probabilities\n\n\n\n\n\n\nMake predictions using a saved binomial model for ICU elective admission\n\n\n\n\n\n\nMake predictions using a saved poisson model for ICU non-elective admission\n\n\n\n\n\n\nWrangle live beds into a simple census\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/icu-tap-emergency-predictions.html",
    "href": "vignettes/icu-tap-emergency-predictions.html",
    "title": "HYLODE",
    "section": "",
    "text": "This notebook shows how to use the models I have saved to ML flow, for the purposes of predicting how many non-elective patients will arrive in the ICU in the 24 hours following 9.15 on a given day.\nOne of three locations can be requested: tower, gwb and wms (all lower case). These have different trained models. You need to retrieve the relevant model from ML flow for the location requested. You would do this be setting model_name and model_version to the saved values (as an example for the tower: MODEL__TAP_NONELECTIVE_TOWER__NAME, MODEL__TAP_NONELECTIVE_TOWER__VERSION need to be set; these should eventually be saved as constants in global settings)\nLogic: - Retrieve a ML model which uses simple date parameters (but could one day be more sophisticated) to generate a mean for a poisson distribution - Usep as a parameter to generate a probability distribution over the number of ICU beds needed by non-elective patients, where the maximum number\nNOTES - we DO make predictions for weekends, unlike for the elective taps - but we donâ€™t differentiate between days of week; just a binary indicator for whether it is a weekday or a weekend - we use a lens here which does not really serve any purpose, other than to one-hot encode whether it is a weekend. However, in future, other covariates (eg the number of patients in the hospital currently, the weather) could be added to the model, and the lens is already in place to add scaling functions\nWe agreed to predict flows at 9.15 and 12.30 pm. However, inspection suggested there is minimal difference between these times so I have dropped the second one and only use 9.15 am\n\n\nThe input data to the models takes the form of of one-row dataframe with the following columns; [â€˜model_functionâ€™, â€˜dateâ€™, â€˜countâ€™, â€˜wkdayâ€™] - model_function - set this to â€˜poissonâ€™ - date - use pd.to_datetime to set format eg pd.to_datetime(â€˜2022-08-08â€™) - count - set this field to zero [this is an artefact of the lens method] - wkday - an integer for whether it is a weekend (value 0) or a weekday (value 1); set this as shown below\n\n\n\n\nimport pkg_resources\n\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([f\"{i.key}=={i.version}\" for i in installed_packages])\n\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\n\nimport os\nimport pickle\nimport tempfile\nfrom pathlib import Path\n\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\n\nimport urllib\n\nfrom hylib import settings\nfrom hylib.dt import LONDON_TZ\nfrom hymind.predict.base import BaseLensPredictor\nfrom patsy import dmatrices\nfrom scipy.stats import poisson\nfrom sqlalchemy import create_engine\n\n\n\n\n\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n\nmlflow_var = os.getenv(\"HYMIND_REPO_TRACKING_URI\")\nmlflow.set_tracking_uri(mlflow_var)\n\nclient = MlflowClient()\n\n\nMODEL__TAP_NONELECTIVE_TOWER__NAME = \"tap_nonelective_tower\"\nMODEL__TAP_NONELECTIVE_TOWER__VERSION = 2\nMODEL__TAP_NONELECTIVE_GWB__NAME = \"tap_nonelective_gwb\"\nMODEL__TAP_NONELECTIVE_GWB__VERSION = 2\nMODEL__TAP_NONELECTIVE_WMS__NAME = \"tap_nonelective_wms\"\nMODEL__TAP_NONELECTIVE_WMS__VERSION = 2\n\n\ndef get_model_details(location):\n\n    if location == \"tower\":\n        model_name, model_version = (\n            MODEL__TAP_NONELECTIVE_TOWER__NAME,\n            MODEL__TAP_NONELECTIVE_TOWER__VERSION,\n        )\n    elif location == \"gwb\":\n        model_name, model_version = (\n            MODEL__TAP_NONELECTIVE_GWB__NAME,\n            MODEL__TAP_NONELECTIVE_GWB__VERSION,\n        )\n    else:\n        model_name, model_version = (\n            MODEL__TAP_NONELECTIVE_WMS__NAME,\n            MODEL__TAP_NONELECTIVE_WMS__VERSION,\n        )\n    return model_name, model_version\n\n\n\n\n\nclass SKProbabilityPredictorPoisson(BaseLensPredictor):\n    def __init__(self, model_name: str, model_version: int) -> None:\n        super().__init__(model_name, model_version)\n        self.model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n        self.expr = self._load_expr(self.model_info.run_id)\n        self.lens = self._load_lens(self.model_info.run_id)\n\n    @staticmethod\n    def _load_expr(run_id: str):\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp_dir = Path(tmp)\n\n            client.download_artifacts(run_id, \"expr\", tmp_dir)\n\n            expr_path = next((tmp_dir / \"expr\").rglob(\"*.txt\"))\n            with open(expr_path, \"rb\") as f:\n                expr = f.read()\n            expr = str(expr, \"utf-8\")\n\n            return expr\n\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n\n        X_df = self.lens.transform(input_df)\n        X_df__, X_df = dmatrices(self.expr, X_df, return_type=\"dataframe\")\n\n        predictions_set_df = self.model.get_prediction(X_df)\n\n        p = predictions_set_df.summary_frame().iloc[0, 0]\n        N = 11\n\n        predictions_df = pd.DataFrame.from_dict(\n            {\n                \"bed_count\": list(range(0, N + 1)),\n                \"probability\": poisson.pmf(list(range(0, N + 1)), p),\n            }\n        )\n\n        predictions_df[\"predict_dt\"] = datetime.now(LONDON_TZ)\n        predictions_df[\"model_name\"] = self.model_name\n        predictions_df[\"model_version\"] = self.model_version\n        predictions_df[\"run_id\"] = self.model_info.run_id\n\n        return predictions_df\n\n\nclass SKProbabilityPredictorStats(BaseLensPredictor):\n    def __init__(self, model_name: str, model_version: int) -> None:\n        super().__init__(model_name, model_version)\n        self.model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n        self.expr = self._load_expr(self.model_info.run_id)\n        self.lens = self._load_lens(self.model_info.run_id)\n        self.input_df = self._is_weekday(input_df)\n        self.input_df = self._elective_list_gt0(input_df)\n\n    def _is_weekday(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if not input_df.iloc[0, 4] in list(range(0, 5)):\n                raise ValueError(\"Date requested is not a weekday\")\n            return input_df\n\n    def _elective_list_gt0(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if input_df.iloc[0, 5] == 0:\n                raise ValueError(\"There are no patients on the elective list\")\n            return input_df\n\n    @staticmethod\n    def _load_expr(run_id: str):\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp_dir = Path(tmp)\n\n            client.download_artifacts(run_id, \"expr\", tmp_dir)\n\n            expr_path = next((tmp_dir / \"expr\").rglob(\"*.txt\"))\n            with open(expr_path, \"rb\") as f:\n                expr = f.read()\n            expr = str(expr, \"utf-8\")\n\n            return expr\n\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n\n        X_df = self.lens.transform(input_df)\n        X_df__, X_df = dmatrices(self.expr, X_df, return_type=\"dataframe\")\n\n        predictions_set_df = self.model.get_prediction(X_df)\n        p = predictions_set_df.summary_frame().iloc[0, 0]\n\n        if input_df.iloc[0, 0] == \"binom\":\n\n            N = input_df.iloc[0, 5]\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": binom.pmf(list(range(0, N + 1)), N, p),\n                }\n            )\n\n        else:\n\n            N = 11\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": poisson.pmf(list(range(0, N + 1)), p),\n                }\n            )\n\n        predictions_df[\"predict_dt\"] = datetime.now(LONDON_TZ)\n        predictions_df[\"model_name\"] = self.model_name\n        predictions_df[\"model_version\"] = self.model_version\n        predictions_df[\"run_id\"] = self.model_info.run_id\n\n        return predictions_df\n\n\n\n\n\n## Create input matrix for model, specifying a date to make a prediction for. The model will return a probability of admission to ICU\n\ndate = \"2022-08-09\"\nto_predict = pd.to_datetime(date)\nlocation = \"tower\"\nmodel_name, model_version = get_model_details(location)\n\ninput_df = pd.DataFrame(\n    np.array([\"poisson\", to_predict, 0])[np.newaxis],\n    columns=[\"model_function\", \"date\", \"count\"],\n)\ninput_df.loc[:, \"wkday\"] = (\n    input_df.loc[:, \"date\"].apply(datetime.weekday).astype(\"object\") <= 4\n)\ninput_df[\"date\"] = input_df[\"date\"].values.astype(np.float)\ninput_df\n\n\nmodel_name, model_version = get_model_details(\"gwb\")\npredictor = SKProbabilityPredictorStats(model_name, model_version)\npredictions_df = predictor.predict(input_df)\n\n\npredictions_df[\"probability\"].values\n\n\n\n\n\nmodel_name, model_version = get_model_details(\"tower\")\n\n\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\nprint(model.summary())\n\n\n# Retrieve the model and the saved lens\n\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\nprint(model.summary())\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n\n    client.download_artifacts(run_id, \"lens\", tmp_dir)\n\n    lens_path = next((tmp_dir / \"lens\").rglob(\"*.pkl\"))\n    with open(lens_path, \"rb\") as f:\n        lens = pickle.load(f)\n\nX_df = lens.transform(input_df)\nX_df__, X_df = dmatrices(expr, X_df, return_type=\"dataframe\")"
  },
  {
    "objectID": "vignettes/wrangle_beds.html",
    "href": "vignettes/wrangle_beds.html",
    "title": "HYLODE",
    "section": "",
    "text": "Run the beds live sql and then wrangle the data into a simple census\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport sqlalchemy as sa\n\nAssume that weâ€™re running from the notebooks directory and need to pull data from ./src/mock/mock.db\n\nsqlite_db = \"../src/mock/mock.db\"\nassert Path(sqlite_db).is_file()\n\n\nengine = sa.create_engine(f\"sqlite:///{sqlite_db}\")\n\n\ndf = pd.read_sql(\"bedsmock\", engine)\n\n\ndf.iloc[0]\n\nSo now you want to return a dataframe with the following characteristics - per department (one row per department) - count number of beds - count number of occupied beds - count number of empty beds - last date of closed discharge from that department (i.e.Â if > 1 week and most beds empty then is the department closed) - number of side rooms?\n\ntemp = (\n    df[\"location_string\"]\n    .str.split(\"^\", expand=True)\n    .rename(columns={0: \"dept\", 1: \"room\", 2: \"bed\"})\n)\nfor s in [\"dept\", \"room\", \"bed\"]:\n    df[s] = temp[s]\n\n\ndel temp\n\nremove null and waiting locations\n\nmask = df[\"bed\"].str.lower().isin([\"null\", \"wait\"])\ndf = df[~mask]\n\n\ndf.shape\n\n\ngroups = df.groupby(\"department\")\n\n\ngroups.get_group(\"GWB L01 CRITICAL CARE\")\n\n\nres = groups.agg(\n    beds=(\"location_id\", \"count\"),\n    patients=(\"occupied\", \"sum\"),\n    last_dc=(\"cvl_discharge\", lambda x: x.max(skipna=True)),\n    modified_at=(\"modified_at\", \"max\"),\n)\nres[\"empties\"] = res[\"beds\"] - res[\"patients\"]\nres[\"opens\"] = res[\"empties\"]  # place holder : need to subtract closed from empties\n\n\nres[\"last_dc\"] = (res[\"modified_at\"] - res[\"last_dc\"]).apply(\n    lambda x: pd.Timedelta.floor(x, \"d\")\n)\n\n\nres[\"closed_temp\"] = pd.DataFrame(\n    [\n        res[\"last_dc\"] > pd.Timedelta(2, \"days\"),\n        res[\"last_dc\"] <= pd.Timedelta(30, \"days\"),\n        res[\"patients\"] == 0,\n    ]\n).T.all(axis=\"columns\")\n\nres[\"closed_perm\"] = pd.DataFrame(\n    [\n        res[\"last_dc\"] > pd.Timedelta(30, \"days\"),\n        res[\"patients\"] == 0,\n    ]\n).T.all(axis=\"columns\")\n\n\nmask = ~res[\"closed_perm\"]\n\nres = res[mask]\nres = res[\n    [\"beds\", \"patients\", \"empties\", \"opens\", \"last_dc\", \"closed_temp\" \"modified_at\"]\n]\n\n\nres\n\n\nfrom typing import List\n\nimport pandas as pd\n\n\ndef _split_location_string(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Splits a location string into dept/room/bed\n    \"\"\"\n    temp = (\n        df[\"location_string\"]\n        .str.split(\"^\", expand=True)\n        .rename(columns={0: \"dept\", 1: \"room\", 2: \"bed\"})\n    )\n    for s in [\"dept\", \"room\", \"bed\"]:\n        df[s] = temp[s]\n    return df\n\n\ndef _remove_non_beds(\n    df: pd.DataFrame, nonbeds: List[str] = [\"null\", \"wait\"]\n) -> pd.DataFrame:\n    \"\"\"\n    Removes non beds e.g. null, wait\n    \"\"\"\n    mask = df[\"bed\"].str.lower().isin(nonbeds)\n    df = df[~mask]\n    return df\n\n\ndef _aggregate_by_department(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Aggregation from location (bed) level to ward level\n    \"\"\"\n    groups = df.groupby(\"department\")\n    # aggregate by dept\n    res = groups.agg(\n        beds=(\"location_id\", \"count\"),\n        patients=(\"occupied\", \"sum\"),\n        last_dc=(\"cvl_discharge\", lambda x: x.max(skipna=True)),\n        modified_at=(\"modified_at\", \"max\"),\n    )\n    # calculate additional numbers\n    res[\"empties\"] = res[\"beds\"] - res[\"patients\"]\n    res[\"opens\"] = res[\"empties\"]  # place holder : need to subtract closed from empties\n    res[\"last_dc\"] = (\n        (res[\"modified_at\"] - res[\"last_dc\"])\n        .apply(lambda x: pd.Timedelta.floor(x, \"d\"))\n        .dt.days\n    )\n\n    # defined closed: temp and perm\n    res[\"closed_temp\"] = pd.DataFrame(\n        [\n            res[\"last_dc\"] > 2,\n            res[\"last_dc\"] <= 30,\n            res[\"patients\"] == 0,\n        ]\n    ).T.all(axis=\"columns\")\n\n    res[\"closed_perm\"] = pd.DataFrame(\n        [\n            res[\"last_dc\"] > 30,\n            res[\"patients\"] == 0,\n        ]\n    ).T.all(axis=\"columns\")\n\n    # drop closed perm\n    mask = ~res[\"closed_perm\"]\n\n    res = res[mask]\n    res = res[\n        [\n            \"beds\",\n            \"patients\",\n            \"empties\",\n            \"opens\",\n            \"last_dc\",\n            \"closed_temp\",\n            \"modified_at\",\n        ]\n    ]\n    res.reset_index(inplace=True)\n    return res\n\n\ndef aggregate_by_department(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Aggregation from location (bed) level to ward level\n    Wrapper function\n    \"\"\"\n    df = _split_location_string(df)\n    df = _remove_non_beds(df)\n    df = _aggregate_by_department(df)\n    return df\n\n\ndf = pd.read_sql(\"bedsmock\", engine)\naggregate_by_department(df)"
  },
  {
    "objectID": "vignettes/beds_v3NHNN.html",
    "href": "vignettes/beds_v3NHNN.html",
    "title": "HYLODE",
    "section": "",
    "text": "2022-07-03vNHNN\nspecifically for NHNN critical care\nworking notes on merging caboodle info on beds with EMAP\naim is to build a reliable view of bed census\nprogrammatically returns a list of locatoins with Caboodle detail\nbut no testing/quality control\nworks in conjunction with add_caboodle2emap_beds.py\nuse to populate base bed definitions for the tower flow"
  },
  {
    "objectID": "vignettes/beds_v3NHNN.html#set-up-incl-database-connections",
    "href": "vignettes/beds_v3NHNN.html#set-up-incl-database-connections",
    "title": "HYLODE",
    "section": "Set-up incl database connections",
    "text": "Set-up incl database connections\n\nimport os\nimport urllib\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\n\n\ndef emap_db() -> Engine:\n    url = \"postgresql+psycopg2://{}:{}@{}:{}/{}\".format(\n        os.getenv(\"EMAP_DB_USER\"),\n        os.getenv(\"EMAP_DB_PASSWORD\"),\n        os.getenv(\"EMAP_DB_HOST\"),\n        os.getenv(\"EMAP_DB_PORT\"),\n        os.getenv(\"EMAP_DB_NAME\"),\n    )\n    engine = create_engine(\n        url, pool_size=4, max_overflow=6, connect_args={\"connect_timeout\": 120}\n    )\n    return engine\n\n\ndef caboodle_db() -> Engine:\n    db_host = os.getenv(\"CABOODLE_DB_HOST\")\n    db_user = os.getenv(\"CABOODLE_DB_USER\")\n    db_password = os.getenv(\"CABOODLE_DB_PASSWORD\")\n    db_port = os.getenv(\"CABOODLE_DB_PORT\")\n    db_name = os.getenv(\"CABOODLE_DB_NAME\")\n    connection_str = f\"mssql+pyodbc://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server\"\n    engine = create_engine(connection_str)\n    return engine\n\n\nemap_engine = emap_db()\ncaboodle_engine = caboodle_db()"
  },
  {
    "objectID": "vignettes/beds_v3NHNN.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "href": "vignettes/beds_v3NHNN.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "title": "HYLODE",
    "section": "Reliable way of joining Caboodle and EMAP bed level data",
    "text": "Reliable way of joining Caboodle and EMAP bed level data\nFirst load EMAP location table into memory\nDo this with no modifications / transformations other than the join\n\nq = \"\"\"\nSELECT \n    lo.location_id, lo.location_string, lo.department_id, lo.room_id, lo.bed_id, dept.name department, dept.speciality, room.name room\nFROM star.location lo\nLEFT JOIN star.department dept ON lo.department_id = dept.department_id\nLEFT JOIN star.room ON lo.room_id = room.room_id\n\"\"\"\ndfe = pd.read_sql_query(q, emap_engine)\ndfe.head()\n\nAnd load Caboodle DepartmentDim into memory\n\nq = \"\"\"\nSELECT \n DepartmentKey\n,BedEpicId\n,Name\n,DepartmentName\n,RoomName\n,BedName\n,IsBed\n,BedInCensus\n,IsDepartment\n,IsRoom\n,IsCareArea\n,DepartmentExternalName\n,DepartmentSpecialty\n,DepartmentType\n,DepartmentServiceGrouper\n,DepartmentLevelOfCareGrouper\n,LocationName\n,ParentLocationName\n,_CreationInstant\n,_LastUpdatedInstant\nFROM dbo.DepartmentDim\n\"\"\"\ndfc = pd.read_sql_query(q, caboodle_engine)\ndfc.head()\n\nNow join these two tables - donâ€™t attempt joins where there is no room/bed level data (b/c theyâ€™re not physical locations - you need a multi-key join - EMAP â€˜nameâ€™ (department.name) joins on to Caboodle â€˜DepartmentNameâ€™ - EMAP â€˜bedâ€™ (derived by splitting location.location_string) joins on to Caboodle â€˜Nameâ€™ - drop â€˜waitâ€™ beds since these duplicate and block a one-to-one merge - try to be rigorous in pd.merge - indicator=True to allow inspection post merge - validate='one_to_one' to throw an error if duplicates found\nNote - sometimes (in Caboodle) DepartmentName and Name are duplicated so pick the most recently â€˜createdâ€™\nusing â€¦\nNow load external code\n\nimport importlib\n\nimport add_caboodle2emap_beds\n\nimportlib.reload(add_caboodle2emap_beds)\nfrom add_caboodle2emap_beds import bed_merge\n\n\ndepartments = [\n    # Built from Tower Report 14 Jun 2022\n    # NAME                         # n emap locations\n    # \"UCH T01 ACUTE MEDICAL\",       # 86\n    # \"UCH T01 ENHANCED CARE\",       # 20\n    # \"UCH T03 INTENSIVE CARE\",      # 37\n    # \"UCH T06 HEAD (T06H)\",         # 27\n    # \"UCH T06 CENTRAL (T06C)\",      # 25\n    # \"UCH T06 SOUTH PACU\",          # 22\n    # \"UCH T06 GYNAE (T06G)\",        # 18\n    # \"UCH T07 NORTH (T07N)\",        # 45\n    # \"UCH T07 CV SURGE\",            # 37\n    # \"UCH T07 SOUTH\",               # 33\n    # \"UCH T07 SOUTH (T07S)\",        # 23\n    # \"UCH T07 HDRU\",                # 20\n    # \"UCH T08 NORTH (T08N)\",        # 28\n    # \"UCH T08 SOUTH (T08S)\",        # 25\n    # \"UCH T08S ARCU\",               #  6\n    # \"UCH T09 SOUTH (T09S)\",        # 34\n    # \"UCH T09 NORTH (T09N)\",        # 32\n    # \"UCH T09 CENTRAL (T09C)\",      # 25\n    # \"UCH T10 SOUTH (T10S)\",        # 34\n    # \"UCH T10 NORTH (T10N)\",        # 32\n    # \"UCH T10 MED (T10M)\",          # 16\n    # \"UCH T11 SOUTH (T11S)\",        # 27\n    # \"UCH T11 NORTH (T11N)\",        # 25\n    # \"UCH T11 EAST (T11E)\",         # 16\n    # \"UCH T11 NORTH (T11NO)\",       #  8\n    # \"UCH T12 SOUTH (T12S)\",        # 32\n    # \"UCH T12 NORTH (T12N)\",        # 23\n    # \"UCH T13 SOUTH (T13S)\",        # 31\n    # \"UCH T13 NORTH ONCOLOGY\",      # 26\n    # \"UCH T13 NORTH (T13N)\",        # 26\n    # \"UCH T14 NORTH TRAUMA\",        # 28\n    # \"UCH T14 NORTH (T14N)\",        # 28\n    # \"UCH T14 SOUTH ASU\",           # 22\n    # \"UCH T14 SOUTH (T14S)\",        # 17\n    # \"UCH T15 SOUTH DECANT\",        # 21\n    # \"UCH T15 SOUTH (T15S)\",        # 21\n    # \"UCH T15 NORTH (T15N)\",        # 16\n    # \"UCH T15 NORTH DECANT\",        # 15\n    # \"UCH T16 NORTH (T16N)\",        # 19\n    # \"UCH T16 SOUTH (T16S)\",        # 18\n    # \"UCH T16 SOUTH WINTER\",        # 17\n    # \"GWB L01 ELECTIVE SURG\",       # 37\n    # \"GWB L01 CRITICAL CARE\",       # 12\n    # \"GWB L02 NORTH (L02N)\",        # 19\n    # \"GWB L02 EAST (L02E)\",         # 19\n    # \"GWB L03 NORTH (L03N)\",        # 19\n    # \"GWB L03 EAST (L03E)\",         # 19\n    # \"GWB L04 NORTH (L04N)\",        # 20\n    # \"GWB L04 EAST (L04E)\",         # 17\n    # \"WMS W04 WARD\",                # 28\n    # \"WMS W03 WARD\",                # 27\n    # \"WMS W02 SHORT STAY\",          # 20\n    # \"WMS W01 CRITICAL CARE\",       # 11\n    \"NHNN C0 NCCU\",\n    \"NHNN C1 NCCU\",\n    # \"NHNN C0 EMERGENCY STROKE UNIT\",\n]\n\n\ndfm = bed_merge(df_emap=dfe, df_caboodle=dfc, departments=departments)\ndfm.head()\n\n\ndfm._merge.value_counts()\n\n\ndfm.to_csv(\"beds.tsv\", sep=\"\\t\", index_label=\"local_id\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hyperlocal Demand Forecasts",
    "section": "",
    "text": "Hyperlocal Demand Forecasts"
  },
  {
    "objectID": "contributor_guide/CONTRIBUTING.html",
    "href": "contributor_guide/CONTRIBUTING.html",
    "title": "HYLODE",
    "section": "",
    "text": "We love contributions! Weâ€™ve compiled this documentation to help you understand our contributing guidelines. If you still have questions, please contact us and weâ€™d be happy to help!\n\n\nPlease read CODE_OF_CONDUCT.md before contributing.\n\n\n\nTo start contributing, open your terminal, and install the required Python packages, and pre-commit hooks using:\npip install -r requirements.txt\npre-commit install\nor the make command:\nmake requirements\nThe pre-commit hooks are a security feature to ensure, for example, no secrets1, large data files, and Jupyter notebook outputs are accidentally committed into the repository. For more information on pre-commit hooks see our documentation.\n\n\n\nWe mainly follow the GDS Way in our code conventions.\n\n\nWe use Git to version control the source code. Please read the GDS Way for details on Git best practice. This includes how to write good commit messages, use git rebase for local branches and git merge --no-ff for merges, as well as using git push --force-with-lease instead of git push -f.\nIf you want to modify the .gitignore files, see the template documentation for further details.\nOur source code is stored on GitHub. Pull requests into main require at least one approved review.\n\n\n\nFor Python code, we follow the GDS Way Python style guide with a line length of 88; the flake8 pre-commit hook should help with this!\n\n\n\nLocal links can be written as normal, but external links should be referenced at the bottom of the Markdown file for clarity. For example:\nUse a [local link to reference the `README.md`](../../README.md) file, but [an external\nlink for GOV.UK][gov-uk].\n\n[gov-uk]: https://www.gov.uk/\nWe also try to wrap Markdown to a line length of 88 characters, but this is not strictly enforced in all cases, for example with long hyperlinks.\n\n\n\n\nTests are written using the pytest framework, with its configuration in the pyproject.toml file. Note, only tests in the tests folder are run. To run the tests, enter the following command in your terminal:\npytest\n\n\nCode coverage of Python scripts is measured using the coverage Python package; its configuration can be found in pyproject.toml. Note coverage only extends to Python scripts in the src folder.\nTo run code coverage, and view it as an HTML report, enter the following command in your terminal:\ncoverage run -m pytest\ncoverage html\nor use the make command:\nmake coverage_html\nThe HTML report can be accessed at htmlcov/index.html.\n\n\n\n\nWe write our documentation in MyST Markdown for use in Sphinx. This is mainly stored in the docs folder, unless itâ€™s more appropriate to store it elsewhere, like this file.\n[Please read our guidance on how to write accessible documentation][docs-write-accessible-documentation], as well as our [guidance on writing Sphinx documentation][docs-write-sphinx-documentation]. This allows you to build the documentation into an accessible, searchable website."
  },
  {
    "objectID": "contributor_guide/pre_commit_hooks.html",
    "href": "contributor_guide/pre_commit_hooks.html",
    "title": "HYLODE",
    "section": "",
    "text": "This repository uses the Python package pre-commit to manage pre-commit hooks. Pre-commit hooks are actions which are run automatically, typically on each commit, to perform some common set of tasks. For example, a pre-commit hook might be used to run any code linting automatically before code is committed, ensuring common code quality.\n\n\nFor this repository, we are using pre-commit for a number of purposes:\n\nchecking for secrets being committed accidentally â€” there is a strict definition of a â€œsecretâ€; and\nchecking for any large files (over 5 MB) being committed.\ncleaning Jupyter notebooks, which means removing all outputs, execution counts, Python kernels, and, for Google Colaboratory (Colab), stripping out user information.\n\nWe have configured pre-commit to run automatically on every commit. By running on each commit, we ensure that pre-commit will be able to detect all contraventions and keep our repository in a healthy state.\n{note} Pre-commit hooks and Google Colab No pre-commit hooks will be run on Google Colab notebooks pushed directly to GitHub. For security reasons, it is recommended that you manually download your notebook, and commit up locally to ensure pre-commit hooks are run on your changes.\n\n\n\nIn order for pre-commit to run, action is needed to configure it on your system.\n\ninstall the pre-commit package into your Python environment from requirements.txt; and\nrun pre-commit install in your terminal to set up pre-commit to run when code is committed.\n\n\n\n\n{note} Secret detection limitations The `detect-secrets` package does its best to prevent accidental committing of secrets, but it may miss things. Instead, focus on good software development practices! See the [definition of a secret for further information](#definition-of-a-secret-according-to-detect-secrets).\nWe use detect-secrets to check that no secrets are accidentally committed. This hook requires you to generate a baseline file if one is not already present within the root directory. To create the baseline file, run the following at the root of the repository:\ndetect-secrets scan > .secrets.baseline\nNext, audit the baseline that has been generated by running:\ndetect-secrets audit .secrets.baseline\nWhen you run this command, youâ€™ll enter an interactive console. This will present you with a list of high-entropy string and/or anything which could be a secret. It will then ask you to verify whether this is the case. This allows the hook to remember false positives in the future, and alert you to new secrets.\n\n\nThe detect-secrets documentation, as of January 2021, says it works:\n\nâ€¦by running periodic diff outputs against heuristically crafted [regular expression] statements, to identify whether any new secret has been committed.\n\nThis means it uses regular expression patterns to scan your code changes for anything that looks like a secret according to the patterns. By definition, there are only a limited number of patterns, so the detect-secrets package cannot detect every conceivable type of secret.\nTo understand what types of secrets will be detected, read the detect-secrets documentation on caveats, and the list of supported plugins. Also, you should use secret variable names with words that will trip the KeywordDetector plugin; see the [DENYLIST variable for the full list of words][detect-secrets-keyword-detector].\n\n\n\nIf pre-commit detects any secrets when you try to create a commit, it will detail what it found and where to go to check the secret.\nIf the detected secret is a false positive, there are two options to resolve this, and prevent your commit from being blocked:\n\ninline allowlisting of false positives (recommended); or\nupdating the .secrets.baseline to include the false positives.\n\nIn either case, if an actual secret is detected (or a combination of actual secrets and false positives), first remove the actual secret. Then following either of these processes.\n\n\nTo exclude a false positive, add a pragma comment such as:\nsecret = \"Password123\"  # pragma: allowlist secret\nor\n#  pragma: allowlist nextline secret\nsecret = \"Password123\"\nIf the detected secret is actually a secret (or other sensitive information), remove the secret and re-commit; there is no need to add any pragma comments.\nIf your commit contains a mixture of false positives and actual secrets, remove the actual secrets first before adding pragma comments to the false positives.\n\n\n\nTo exclude a false positive, you can also update the .secrets.baseline by repeating the same two commands as in the initial setup.\nDuring auditing, if the detected secret is actually a secret (or other sensitive information), remove the secret and re-commit. There is no need to update the .secrets.baseline file in this case.\nIf your commit contains a mixture of false positives and actual secrets, remove the actual secrets first before updating and auditing the .secrets.baseline file.\n\n\n\n\n\nIt may be necessary or useful to keep certain output cells of a Jupyter notebook, for example charts or graphs visualising some set of data. To do this, according to the documentation for the nbstripout package, either:\n\nadd a keep_output tag to the desired cell; or\nadd \"keep_output\": true to the desired cellâ€™s metadata.\n\nYou can access cell tags or metadata in Jupyter by enabling the â€œTagsâ€ or â€œEdit Metadataâ€ toolbar (View > Cell Toolbar > Tags; View > Cell Toolbar > Edit Metadata).\nFor the tags approach, enter keep_output in the text field for each desired cell, and press the â€œAdd tagâ€ button. For the metadata approach, press the â€œEdit Metadataâ€ button on each desired cell, and edit the metadata to look like this:\n{\n  \"keep_output\": true\n}\nThis will tell the hook not to strip the resulting output of the desired cell(s), allowing the output(s) to be committed.\n```{note} Tags and metadata on Google Colab Currently (March 2020) there is no way to add tags and/or metadata to Google Colab notebooks.\nItâ€™s strongly suggested that you download the Colab as a .ipynb file, and edit tags and/or metadata using Jupyter before committing the code if you want to keep some outputs. ```"
  },
  {
    "objectID": "contributor_guide/CODE_OF_CONDUCT.html",
    "href": "contributor_guide/CODE_OF_CONDUCT.html",
    "title": "HYLODE",
    "section": "",
    "text": "Contributors to this repository hosted by HYLODE are expected to follow the Contributor Covenant Code of Conduct, and those working within Her Majestyâ€™s Government are also expected to follow the Civil Service Code.\n\n\nContributors working within Her Majestyâ€™s Government must review the Civil Service Code, and are expected to follow it in their contributions.\n\n\n\n\n\nWhere this Code of Conduct says:\n\nâ€œProjectâ€, we mean this hyui GitHub repository;\nâ€œMaintainerâ€, we mean the HYLODE organisation owners; and\nâ€œLeadershipâ€, we mean both HYLODE organisation owners, line managers, and other leadership within the UCL-HAL.\n\n\n\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project, and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behaviour that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behaviour by participants include:\n\nThe use of sexualised language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing othersâ€™ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behaviour and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviours that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting using an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team at doc@steveharris.me. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the projectâ€™s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct/, and the alphagov Code of Conduct available at https://github.com/alphagov/.github/blob/main/CODE_OF_CONDUCT.md."
  },
  {
    "objectID": "contributor_guide/updating_gitignore.html",
    "href": "contributor_guide/updating_gitignore.html",
    "title": "HYLODE",
    "section": "",
    "text": "Updating the .gitignore file\nThe .gitignore used in this repository was created with generic exclusions from gitignore.io, with project-specific exclusions listed afterwards.\nIf you want to add exclusions for new programming languages and/or IDEs, use the first line to recreate the generic exclusions from gitignore.io. Add all other project-specific exclusions afterwards."
  }
]