[
  {
    "objectID": "contributor_guide/CODE_OF_CONDUCT.html",
    "href": "contributor_guide/CODE_OF_CONDUCT.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Contributors to this repository hosted by HYLODE are expected to follow the Contributor Covenant Code of Conduct, and those working within Her Majesty’s Government are also expected to follow the Civil Service Code.\n\n\nContributors working within Her Majesty’s Government must review the Civil Service Code, and are expected to follow it in their contributions.\n\n\n\n\n\nWhere this Code of Conduct says:\n\n“Project”, we mean this hyui GitHub repository;\n“Maintainer”, we mean the HYLODE organisation owners; and\n“Leadership”, we mean both HYLODE organisation owners, line managers, and other leadership within the UCL-HAL.\n\n\n\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project, and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behaviour that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behaviour by participants include:\n\nThe use of sexualised language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behaviour and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviours that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting using an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team at doc@steveharris.me. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct/, and the alphagov Code of Conduct available at https://github.com/alphagov/.github/blob/main/CODE_OF_CONDUCT.md."
  },
  {
    "objectID": "contributor_guide/CONTRIBUTING.html",
    "href": "contributor_guide/CONTRIBUTING.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Footnotes\n\n\nOnly secrets of specific patterns are detected by the pre-commit hooks.↩︎"
  },
  {
    "objectID": "contributor_guide/updating_gitignore.html",
    "href": "contributor_guide/updating_gitignore.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "The .gitignore used in this repository was created with generic exclusions from gitignore.io, with project-specific exclusions listed afterwards.\nIf you want to add exclusions for new programming languages and/or IDEs, use the first line to recreate the generic exclusions from gitignore.io. Add all other project-specific exclusions afterwards."
  },
  {
    "objectID": "contributor_guide/pre_commit_hooks.html",
    "href": "contributor_guide/pre_commit_hooks.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "This repository uses the Python package pre-commit to manage pre-commit hooks. Pre-commit hooks are actions which are run automatically, typically on each commit, to perform some common set of tasks. For example, a pre-commit hook might be used to run any code linting automatically before code is committed, ensuring common code quality.\n\n\nFor this repository, we are using pre-commit for a number of purposes:\n\nchecking for secrets being committed accidentally — there is a strict definition of a “secret”; and\nchecking for any large files (over 5 MB) being committed.\ncleaning Jupyter notebooks, which means removing all outputs, execution counts, Python kernels, and, for Google Colaboratory (Colab), stripping out user information.\n\nWe have configured pre-commit to run automatically on every commit. By running on each commit, we ensure that pre-commit will be able to detect all contraventions and keep our repository in a healthy state.\n{note} Pre-commit hooks and Google Colab No pre-commit hooks will be run on Google Colab notebooks pushed directly to GitHub. For security reasons, it is recommended that you manually download your notebook, and commit up locally to ensure pre-commit hooks are run on your changes.\n\n\n\nIn order for pre-commit to run, action is needed to configure it on your system.\n\ninstall the pre-commit package into your Python environment from requirements.txt; and\nrun pre-commit install in your terminal to set up pre-commit to run when code is committed.\n\n\n\n\n{note} Secret detection limitations The `detect-secrets` package does its best to prevent accidental committing of secrets, but it may miss things. Instead, focus on good software development practices! See the [definition of a secret for further information](#definition-of-a-secret-according-to-detect-secrets).\nWe use detect-secrets to check that no secrets are accidentally committed. This hook requires you to generate a baseline file if one is not already present within the root directory. To create the baseline file, run the following at the root of the repository:\ndetect-secrets scan > .secrets.baseline\nNext, audit the baseline that has been generated by running:\ndetect-secrets audit .secrets.baseline\nWhen you run this command, you’ll enter an interactive console. This will present you with a list of high-entropy string and/or anything which could be a secret. It will then ask you to verify whether this is the case. This allows the hook to remember false positives in the future, and alert you to new secrets.\n\n\nThe detect-secrets documentation, as of January 2021, says it works:\n\n…by running periodic diff outputs against heuristically crafted [regular expression] statements, to identify whether any new secret has been committed.\n\nThis means it uses regular expression patterns to scan your code changes for anything that looks like a secret according to the patterns. By definition, there are only a limited number of patterns, so the detect-secrets package cannot detect every conceivable type of secret.\nTo understand what types of secrets will be detected, read the detect-secrets documentation on caveats, and the list of supported plugins. Also, you should use secret variable names with words that will trip the KeywordDetector plugin; see the [DENYLIST variable for the full list of words][detect-secrets-keyword-detector].\n\n\n\nIf pre-commit detects any secrets when you try to create a commit, it will detail what it found and where to go to check the secret.\nIf the detected secret is a false positive, there are two options to resolve this, and prevent your commit from being blocked:\n\ninline allowlisting of false positives (recommended); or\nupdating the .secrets.baseline to include the false positives.\n\nIn either case, if an actual secret is detected (or a combination of actual secrets and false positives), first remove the actual secret. Then following either of these processes.\n\n\nTo exclude a false positive, add a pragma comment such as:\nsecret = \"Password123\"  # pragma: allowlist secret\nor\n#  pragma: allowlist nextline secret\nsecret = \"Password123\"\nIf the detected secret is actually a secret (or other sensitive information), remove the secret and re-commit; there is no need to add any pragma comments.\nIf your commit contains a mixture of false positives and actual secrets, remove the actual secrets first before adding pragma comments to the false positives.\n\n\n\nTo exclude a false positive, you can also update the .secrets.baseline by repeating the same two commands as in the initial setup.\nDuring auditing, if the detected secret is actually a secret (or other sensitive information), remove the secret and re-commit. There is no need to update the .secrets.baseline file in this case.\nIf your commit contains a mixture of false positives and actual secrets, remove the actual secrets first before updating and auditing the .secrets.baseline file.\n\n\n\n\n\nIt may be necessary or useful to keep certain output cells of a Jupyter notebook, for example charts or graphs visualising some set of data. To do this, according to the documentation for the nbstripout package, either:\n\nadd a keep_output tag to the desired cell; or\nadd \"keep_output\": true to the desired cell’s metadata.\n\nYou can access cell tags or metadata in Jupyter by enabling the “Tags” or “Edit Metadata” toolbar (View > Cell Toolbar > Tags; View > Cell Toolbar > Edit Metadata).\nFor the tags approach, enter keep_output in the text field for each desired cell, and press the “Add tag” button. For the metadata approach, press the “Edit Metadata” button on each desired cell, and edit the metadata to look like this:\n{\n  \"keep_output\": true\n}\nThis will tell the hook not to strip the resulting output of the desired cell(s), allowing the output(s) to be committed.\n```{note} Tags and metadata on Google Colab Currently (March 2020) there is no way to add tags and/or metadata to Google Colab notebooks.\nIt’s strongly suggested that you download the Colab as a .ipynb file, and edit tags and/or metadata using Jupyter before committing the code if you want to keep some outputs. ```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HyUI documentation",
    "section": "",
    "text": "User interface for the HYLODE project Much of the project structure is based on thegovcookiecutter template project but is adapted for the development of Plotly Dash apps within a hospital environment. A template application would be split into a frontend and a backend with communication between the two via HTTP, and the project orchestrated by docker-compose.\nThis file (./readme.md) is at the root of the project, but the application code including the backend is in ./src/api, and the application itself is kept in ./src/apps. An annotated figure of the directory structure is shown below."
  },
  {
    "objectID": "index.html#deployment",
    "href": "index.html#deployment",
    "title": "HyUI documentation",
    "section": "Deployment",
    "text": "Deployment\n\nQuick start\nFrom the commandline of the GAE\ngit clone https://github.com/HYLODE/HyUi.git\ncd HyUi\ncp .env.example .env\n# Now hand edit the .env file with usernames/passwords\n# Set ENV=prod (rather than dev)\npytest # OPTIONAL\ndocker-compose up -d --build && docker-composes logs -f\nGo to http://my-host-name:8094/docs for the API\nGo to http://my-host-name:8095 for the dashboard landing page\nRefer to Deployment for more details.\n\n\nDevelopment\nRefer to Dev Setup for details on setting up development environments.\n\n\nDevelopment (Hospital)\nThere are two tasks that must happen within the hospital environment: (1) preparing realistic mock data (2) deployment. The installation steps differ because here we do not have sudo (root) access (admin privileges). This means work must be performed using a combination of the default command line tools and docker.\nYOU MUST NOW PREPARE YOUR DATA MODEL IN ./src/api/models.py This is a quality control step that ensures the data you handle is of the correct type. let’s generalise the naming so that query is matched to results which has rows and results is a pydantic / sqlmodel class by hand, specify as per … https://sqlmodel.tiangolo.com/tutorial/create-db-and-table/\nA simple Pandas dataframe with two string columns and a timestamp.\n>>> df.types\nfirstname                          object\nlastname                           object\nadmission_datetime                 datetime64[ns]\nThe equivalent SQLModel. Note that firstname is optional but that lastname and admission_datetime are not.\nfrom sqlmodel import SQLModel\nfrom datetime import datetime\n\nclass ResultsBase(SQLModel):\n    \"\"\"\n    Generic results class to hold data returned from\n    the SQL query or the API\n    \"\"\"\n    firstname: Optional[str]\n    lastname: str\n    admission_datetime: datetime\nYou can also use the @validator decorator function to add additional validation.\n\n\nProd deployment\nSet the environment variable to prod, then run docker-compose.\nexport ENV=prod\npytest\ndocker-compose up -d --build && docker-compose logs -f\nYou will need create a local ./.secrets file with database credentials so preferably set the ENV to prod there.\n\n\nOrchestrating front and back end\nIMPORTANT: ensure you have created a ./.secrets file with at least the same info as the ./.secrets.example version\ndocker-compose down\ndocker-compose up -d --build && docker-compose logs -f"
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "HyUI documentation",
    "section": "Project structure",
    "text": "Project structure\n.\n|\n|-- readme.md           // this file and the project root\n|-- Makefile            // standard project commands to be run from the shell\n|-- config files        // e.g. requirements.txt, .gitignore\n|-- .secrets            // e.g. .secrets etc excluded from version control\n|-- secrets.example     // an example version of .secrets above\n|-- docker-compose.yml  // orchestrate front/backend\n|-- src\n    |-- frontend\n        |-- Dockerfile\n        |-- dash_app.py\n    |-- backend\n        |-- Dockerfile\n        |-- query.sql   // SQL used to drive the backend API\n|-- synth               // Synthetic data generation for testing\n    |-- work\n|-- tests\n|-- docs\n|-- data\n|-- outputs\n|-- notebooks           // jupyter notebooks etc\n|-- try                 // ideas playground"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "HyUI documentation",
    "section": "Getting started",
    "text": "Getting started\nTo start using this project, first make sure your system meets its requirements.\nTo be added.\n\nRequirements\n{note} Requirements for contributors [Contributors have some additional requirements][contributing]!\n\nPython 3.6.1+ installed\na .secrets file with the required secrets and credentials\nload environment variables from .envrc\n\nTo install the Python requirements, open your terminal and enter:\npip install -r requirements.txt"
  },
  {
    "objectID": "index.html#required-secrets-and-credentials",
    "href": "index.html#required-secrets-and-credentials",
    "title": "HyUI documentation",
    "section": "Required secrets and credentials",
    "text": "Required secrets and credentials\nTo run this project, you need a .secrets file with secrets/credentials as environmental variables. The secrets/credentials should have the following environment variable name(s):\n\n\n\n\n\n\n\n\nSecret/credential\nEnvironment variable name\nDescription\n\n\n\n\nSecret 1\nSECRET_VARIABLE_1\nPlain English description of Secret 1.\n\n\nCredential 1\nCREDENTIAL_VARIABLE_1\nPlain English description of Credential 1.\n\n\n\nOnce you’ve added, load these environment variables using .envrc."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "HyUI documentation",
    "section": "Licence",
    "text": "Licence\nUnless stated otherwise, the codebase is released under the MIT License. This covers both the codebase and any sample code in the documentation. The documentation is © Crown copyright and available under the terms of the Open Government 3.0 licence."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "HyUI documentation",
    "section": "Contributing",
    "text": "Contributing\nIf you want to help us build, and improve hyui, view our contributing guidelines."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "HyUI documentation",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis project structure is based on the govcookiecutter template project."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "developer/environment-variables.html",
    "href": "developer/environment-variables.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "We use direnv to load environment variables, as these are only loaded when inside the project folder. This can prevent accidental conflicts with identically named variables.\n\n\nTo load the environment variables, first follow the direnv installation instructions, and make sure you have a .secrets file to store secrets and credentials. Then:\n\nOpen your terminal;\nNavigate to the project folder; and\n\nYou should see the following message:\ndirenv: error .envrc is blocked. Run `direnv allow` to approve its content.\n\nAllow direnv.\ndirenv allow\n\nYou only need to do this once, and again each time .envrc and .secrets are modified.\n\n\nThese instructions assume you are running on macOS with administrator privileges using a bash terminal. For other ways of installing direnv, and its shell hooks, consult the direnv documentation.\n\nOpen your terminal;\nInstall direnv using Homebrew;\nbrew install direnv\nAdd the shell hooks to your .bash_profile;\necho 'eval \"$(direnv hook bash)\"' >> ~/.bash_profile\nCheck that the shell hooks have been added correctly; and\ncat ~/.bash_profile\n\nThis should display eval \"$(direnv hook bash)\"\n\nRestart your terminal.\n\n\n\n\n\nSecrets and credentials must be stored in the .secrets file. This file is not version-controlled, so no secrets should be committed to GitHub.\nIn your terminal navigate to the root folder, and create a .secrets file.\ntouch .secrets\nOpen this new .secrets file using your preferred text editor, and add any secrets as environmental variables. For example, to add a JSON credentials file for Google BigQuery, save the following changes to .secrets.\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/credentials.json\"\nOnce complete, make sure the .secrets file has the following line uncommented out:\nsource_env \".secrets\"\nThis ensures direnv loads the .secrets file using .envrc without version-controlling .secrets."
  },
  {
    "objectID": "developer/synthetic-data.html",
    "href": "developer/synthetic-data.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Be careful! We will use the tooling provided by the Synthetic Data Lab from a JupyterLab in a browser on a hospital machine. You will need access to the GAE to run this.\n\n\n\n\nEnsure that your database credentials are stored in the ./.secrets file. From the GAE commandline, navigate to the synth directory (cd ./synth), then use the Makefile commands as follows\n\nmake mock1build to build a docker image with JupyterLab and sdv pre-installed.\nmake mock2run to spin up JupyterLab (e.g. Working from GAE07 this will be at  but the URL server will depend on the GAE).\nmake mock3copyin This will copy the example notebook synth_test_data.ipynb into the ./synth/portal directory that is attached to the JupyterNotebook. Now open the example notebook synth_test_data.ipynb using JupyterLab and work through the steps. Create your SQL query and save as query_live.sql file must return a SELECT statement. Save just the synthesised mock data to mock.h5, and the query (query_live.sql). Be careful and ensure that you specify ‘fakers’ for all direct identifiers. We recommend the four eyes approach wherein a second person reviews your work before an export.\nmake mock4copyout to copy just the query and the synthetic data. Do not copy the notebook out of the portal directory unless you are sure you have stripped all personally identifiable data (i.e. clear all cells before saving).\nmake mock5stop to stop the JupyterLab instance and clear up\n\n\n\n\nThis is similar to the steps above but does not depend on the query or database credentials. You are likely to need to use the Python requests library to get the data that will be used by sdv.\n\n\n\n\nWithout forcing this specification, then SDV treats columns as ‘categories’ and shuffles / re-arranges but the original data will appear.\nDefine fields that contain PII and need faking (see the sketchy documentation here and the Faker Documentation for a full list of providers. Here is a brief example that specifies Fakers for name and date of birth. Note that you must pass arguments to a faker as a list. NB: sdv also doesn’t always recognise the columns correctly. Here we specify data_of_birth explicitly as a date whilst working on the larger task of defining columns that contain PII. See field details\nExample specification to force faking for PII fields within the data\nfields = {\n    'dob': {\n        'type': 'datetime',\n        'format': '%Y-%m-%d',\n        'pii': True,\n        # the 'pii_category' key defines the Faker function name (method)\n        'pii_category': \"date_of_birth\",\n    },\n    'admission_age_years': {\n        'type': 'numerical',\n        'pii': True,\n        'pii_category': ['random_number', 2 ]\n    },\n    'name': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': 'name'\n    },\n    'mrn': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': ['random_number', 8 ]\n    },\n    'csn': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': ['numerify', '10########' ]\n    },\n    'postcode': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': 'postcode',\n        'pii_locales': ['en_GB'],\n    },\n}"
  },
  {
    "objectID": "developer/deployment.html",
    "href": "developer/deployment.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "From the commandline of the GAE\ngit clone https://github.com/HYLODE/HyUi.git\ncd HyUi\ncp .env.example .env\n# Now hand edit the .env file with usernames/passwords\n# Set ENV=prod (rather than dev)\npytest # OPTIONAL\ndocker-compose up -d --build && docker-composes logs -f\nGo to http://my-host-name:8094/docs for the API\nGo to http://my-host-name:8095 for the dashboard landing page"
  },
  {
    "objectID": "developer/setup.html",
    "href": "developer/setup.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Read this if\n\nyou are able to code in Python\nyou already have access to data at UCLH (e.g. EMAP databases or a HYLODE API)\nyou want to build a data visualisation using Plotly Dash\n\n\n\nThere are 2 primary environments needed for development: local machine and a live machine\n\n\nYour own laptop etc. without access to personally identifiable information (PII) etc. You wish to be able to build and run applications with test data.\n\n\n\nAn NHS machine or similar within sensitive environment with access to PII. You wish to be able to deploy the actual application or generate synthetic data to use in a local environment.\n\n\n\n\n\n\n\nPython 3.6.1+ installed\nInstall direnv, following these instructions\nInstall conda miniforge (e.g. brew install miniforge)\n\n\n\n\n\n\n\nNote\n\n\n\nWe assume that the majority of the development work will be on your own machine. You will therefore need to set-up your own dev environment. I have been using conda miniforge because this has the best compatibility with the ARM processor on the Mac. I think it should also work on other machines but I have not tested this.\n\n\n\n\n\ngit clone https://github.com/HYLODE/HyUi.git\ncd HyUi\ncp .env.example .env\n\n\n\nconda env create --file=./dev/environment.yml\nconda activate hyui\nThen confirm your set-up is OK\npytest -m smoke src/tests/smoke\n\n\n\nAutomatically activate your conda environment when you cd into the hyui directory\nhttps://github.com/theskumar/python-dotenv installed python-dotenv\npip install python-dotenv\nand https://direnv.net\nbrew install direnv\ntouch .envrc\ndirenv allow .\nadd the following to .zshrc\neval \"$(direnv hook zsh)\"\n\n\n\nsee docs/contributor_guide/pre_commit_hooks.md need to run this by hand the first time\npre-commit install\npre-commit run\n\n\n\nDepending on your dev computer, you may not want to run HyUI using Docker (e.g. Apple Silicon laptops experience very high memory use when running docker.)\n\n\nRun docker-compose up -d to stand up both backend and frontend services using docker.\nWARNING: you may need docker compose rather than docker-compose depending on your docker version WARNING: docker services depend on a valid .env file (specifically to hold the postgres credentials)\n\n\n\ncd ./src\nuvicorn api.main:app --reload --workers 4 --host 0.0.0.0 --port 8092\nthen navigate to http://localhost:8092/docs to view the API documentation\n\n\n\ncd ./src\nENV=dev DOCKER=False python apps/app.py\nthen navigate to http://localhost:8093 to view the app\n\n\n\n\n\nOnce you’re up and running, you can review the development workflow in Development Workflow\n\n\n\nSee Hylode HySys"
  },
  {
    "objectID": "developer/documentation.html",
    "href": "developer/documentation.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Set up for editing and building documentation All based around Quarto Documentation in ./docs\n\ninstalled basic python and jupyter extensions necessary including those for jupyter\ninstalled quarto\ninstalled the VSCode extension\n\nWorkflow\ncd docs\nquarto preview\nDon’t forget that changes to config (e.g. _quarto.yml may need you to rerun quarto render to ensure the whole site is correctly rebuilt)\nWhen you push to either dev or main branches, then a GitHub Action (see .github/workflows/publish.yml) will be triggered that should publish the current documentation to https://hylode.github.io/HyUi/."
  },
  {
    "objectID": "developer/workflow.html",
    "href": "developer/workflow.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Note\n\n\n\n\nPersona\n\ndeveloper reading this who wants to deploy an dash app Setting\n\n\nworking on their own lap top with access to UCLH data and machines"
  },
  {
    "objectID": "developer/workflow.html#make-synthetic-version-of-the-data",
    "href": "developer/workflow.html#make-synthetic-version-of-the-data",
    "title": "HYLODE documentation",
    "section": "1. Make synthetic version of the data",
    "text": "1. Make synthetic version of the data\nWe imagine that the developer has the appropriate permissions to view the raw data including patient identifiable information (either themselves, or in partnership with a colleague). A justification for this position is [here][provisioning]. Practically, this means early interactive data exploration using the UCLH Datascience Desktop and similar tools, and in turn access to EMAP and Clarity/Caboodle.\nThis should generate an initial data specification, and this can be used to generate synthetic data. The synthetic data can then be used to drive the rest of the pathway."
  },
  {
    "objectID": "developer/workflow.html#develop-with-synthetic-data",
    "href": "developer/workflow.html#develop-with-synthetic-data",
    "title": "HYLODE documentation",
    "section": "2. Develop with synthetic data",
    "text": "2. Develop with synthetic data"
  },
  {
    "objectID": "developer/workflow.html#write-some-tests-and-quality-control",
    "href": "developer/workflow.html#write-some-tests-and-quality-control",
    "title": "HYLODE documentation",
    "section": "3. Write some tests and quality control",
    "text": "3. Write some tests and quality control"
  },
  {
    "objectID": "developer/workflow.html#update-the-plot-to-run-live",
    "href": "developer/workflow.html#update-the-plot-to-run-live",
    "title": "HYLODE documentation",
    "section": "4. Update the plot to run live",
    "text": "4. Update the plot to run live"
  },
  {
    "objectID": "developer/workflow.html#allow-inspection-over-the-last-months",
    "href": "developer/workflow.html#allow-inspection-over-the-last-months",
    "title": "HYLODE documentation",
    "section": "5. Allow inspection over the last months",
    "text": "5. Allow inspection over the last months"
  },
  {
    "objectID": "developer/workflow.html#split-by-specialty",
    "href": "developer/workflow.html#split-by-specialty",
    "title": "HYLODE documentation",
    "section": "6. Split by specialty",
    "text": "6. Split by specialty"
  },
  {
    "objectID": "developer/workflow.html#prepare-synthetic-data-at-uclh",
    "href": "developer/workflow.html#prepare-synthetic-data-at-uclh",
    "title": "HYLODE documentation",
    "section": "Prepare synthetic data at UCLH",
    "text": "Prepare synthetic data at UCLH\n\nLog on to a UCLH machine\nOpen a browser (e.g. Chrome)\nLog on to the HyUi JupyterLab instance (e.g. http://uclvlddpragae07:8091/)\nEither\n\nIf you’re working with a SQL script then follow the steps in synth_test_data_consults.ipynb\nIf you’re working with a local HTTP endpoint (e.g. HYLODE API for the census http://uclvlddpragae07:5006/emap/icu/census/T03/) follow the steps in synth_test_data_census.ipynb\n\nCopy out your synthetic data\n\nSee the Synthetic Data Preparation docs."
  },
  {
    "objectID": "developer/workflow.html#set-up-local-api",
    "href": "developer/workflow.html#set-up-local-api",
    "title": "HYLODE documentation",
    "section": "Set-up local API",
    "text": "Set-up local API\nmake a local API module create the __init__.py with reference to routes the directory should contain\napi/mymodule\n|\n|-- __init__.py         // define project wide variables\n|-- mock.h5             // synthetic data create above for mocking\n|-- mock.sql            // query to run against mock database\n|-- model.py            // pydantic (SQLmodel) to define data\nadd a new route to config.settings.ModuleNames\nclass ModuleNames(str, Enum):\n    \"\"\"\n    Class that defines routes for each module\n    \"\"\"\n\n    consults = \"consults\"\n    sitrep = \"sitrep\"\n    mymodule = \"mymodule\""
  },
  {
    "objectID": "developer/workflow.html#set-up-local-testing",
    "href": "developer/workflow.html#set-up-local-testing",
    "title": "HYLODE documentation",
    "section": "Set-up local testing",
    "text": "Set-up local testing\nthen run mock.py (and this will insert the mock data into the local sqlite database) then either make api or uvicorn api.main:app --reload --workers 4 --host 0.0.0.0 --port 8092 navigate to http://localhost:8092/docs to check that it works\nthen create a module for testing (just duplicate an existing one and adapt) you will need a local conftest.py to set up the mockdata"
  },
  {
    "objectID": "developer/workflow.html#set-up-local-dash-app",
    "href": "developer/workflow.html#set-up-local-dash-app",
    "title": "HYLODE documentation",
    "section": "Set-up local Dash app",
    "text": "Set-up local Dash app\nthen create an app in src/apps/mymodule ideally import headers and other attributes (from src/apps/index.py) for consistent styling register the new routes in src/apps/index.py"
  },
  {
    "objectID": "developer/workflow.html#debugging-after-deployment-in-the-live-environment",
    "href": "developer/workflow.html#debugging-after-deployment-in-the-live-environment",
    "title": "HYLODE documentation",
    "section": "Debugging after deployment in the live environment",
    "text": "Debugging after deployment in the live environment\nSince we don’t have root access, then the deployment happens via docker. This means that the debugging must also happen inside a live docker container. The following seems to work.\nEnsure that your docker-compose file includes these options for the image that you wish to debug.\nIf this is your docker-compose config for the api service:\n\n  api:\n    build:\n      context: ./src\n      dockerfile: Dockerfile.api\n...\nthen add these lines\n    stdin_open: true\n    tty: true\n\nRun docker-compose up -d --build as usual.\nFind the ID of the approprite container via docker container ps | grep hyui (e.g. abc123)\nUse that ID to run docker attach abc123. Don’t be put off if nothing seems to happen.\nWhenever the python debugger is triggered (e.g. via a breakpoint import pdb; pdb.set_trace()) the container will return control to the terminal above.\nTo exit without killing the container then use CONTROL-P, CONTROL-Q instead of CONTROL-C\n\nAll this via https://blog.lucasferreira.org/howto/2017/06/03/running-pdb-with-docker-and-gunicorn.html with tips for the Python Debugger via https://realpython.com/python-debugging-pdb/."
  }
]