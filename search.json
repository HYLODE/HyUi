[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HyUI documentation",
    "section": "",
    "text": "User interface for the HYLODE project Much of the project structure is based on thegovcookiecutter template project but is adapted for the development of Plotly Dash apps within a hospital environment. A template application would be split into a frontend and a backend with communication between the two via HTTP, and the project orchestrated by docker-compose.\nThe file (./readme.md) is at the root of the project, but the application code including the backend is in ./src/api, and the application itself is kept in ./src/apps. An annotated figure of the directory structure is shown below."
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "HyUI documentation",
    "section": "Development",
    "text": "Development\nRefer to Dev Setup for details on setting up development environments."
  },
  {
    "objectID": "index.html#development-hospital",
    "href": "index.html#development-hospital",
    "title": "HyUI documentation",
    "section": "Development (Hospital)",
    "text": "Development (Hospital)\nThere are two tasks that must happen within the hospital environment:\n\npreparing realistic mock data\ndeployment\n\nThe installation steps differ (from local development) because here we do not have sudo (root) access (admin privileges). This means work must be performed using a combination of the default command line tools and docker."
  },
  {
    "objectID": "index.html#deployment-hospital",
    "href": "index.html#deployment-hospital",
    "title": "HyUI documentation",
    "section": "Deployment (hospital)",
    "text": "Deployment (hospital)\nSet the environment variable to prod, then run docker-compose.\nexport ENV=prod\npytest\ndocker-compose up -d --build && docker-compose logs -f\nYou will need create a local ./.env file with database credentials so preferably set the ENV to prod there.\n\nOrchestrating front and back end\nIMPORTANT: ensure you have created a ./.env file with at least the same info as the ./.env.example version\ndocker-compose down\ndocker-compose up -d --build && docker-compose logs -f"
  },
  {
    "objectID": "index.html#project-structure",
    "href": "index.html#project-structure",
    "title": "HyUI documentation",
    "section": "Project structure",
    "text": "Project structure\n.\n|\n|-- readme.md           // this file and the project root\n|-- Makefile            // standard project commands to be run from the shell\n|-- config files        // e.g. requirements.txt, .gitignore\n|-- .secrets            // e.g. .secrets etc excluded from version control\n|-- secrets.example     // an example version of .secrets above\n|-- docker-compose.yml  // orchestrate front/backend\n|-- src\n    |-- frontend\n        |-- Dockerfile\n        |-- dash_app.py\n    |-- backend\n        |-- Dockerfile\n        |-- query.sql   // SQL used to drive the backend API\n|-- synth               // Synthetic data generation for testing\n    |-- work\n|-- tests\n|-- docs                // this Quarto website\n|-- data\n|-- outputs\n|-- notebooks           // jupyter notebooks etc\n|-- try                 // ideas playground"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "HyUI documentation",
    "section": "Contributing",
    "text": "Contributing\nIf you want to help us build, and improve hyui, view our contributing guidelines."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "HyUI documentation",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis project structure is based on the govcookiecutter template project."
  },
  {
    "objectID": "developer/documentation.html",
    "href": "developer/documentation.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Set up for editing and building documentation All based around Quarto Documentation in ./docs\n\ninstalled basic python and jupyter extensions necessary including those for jupyter\ninstalled quarto\ninstalled the VSCode extension\n\nWorkflow\ncd docs\nquarto preview\nDon’t forget that changes to config (e.g. _quarto.yml may need you to rerun quarto render to ensure the whole site is correctly rebuilt)\nWhen you push to either dev or main branches, then a GitHub Action (see .github/workflows/publish.yml) will be triggered that should publish the current documentation to https://hylode.github.io/HyUi/."
  },
  {
    "objectID": "developer/synthetic-data.html",
    "href": "developer/synthetic-data.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Be careful! We will use the tooling provided by the Synthetic Data Lab from a JupyterLab in a browser on a hospital machine. You will need access to the GAE to run this.\n\n\n\n\nEnsure that your database credentials are stored in the ./.secrets file. From the GAE commandline, navigate to the synth directory (cd ./synth), then use the Makefile commands as follows\n\nmake mock1build to build a docker image with JupyterLab and sdv pre-installed.\nmake mock2run to spin up JupyterLab (e.g. Working from GAE07 this will be at  but the URL server will depend on the GAE).\nmake mock3copyin This will copy the example notebook synth_test_data.ipynb into the ./synth/portal directory that is attached to the JupyterNotebook. Now open the example notebook synth_test_data.ipynb using JupyterLab and work through the steps. Create your SQL query and save as query_live.sql file must return a SELECT statement. Save just the synthesised mock data to mock.h5, and the query (query_live.sql). Be careful and ensure that you specify ‘fakers’ for all direct identifiers. We recommend the four eyes approach wherein a second person reviews your work before an export.\nmake mock4copyout to copy just the query and the synthetic data. Do not copy the notebook out of the portal directory unless you are sure you have stripped all personally identifiable data (i.e. clear all cells before saving).\nmake mock5stop to stop the JupyterLab instance and clear up\n\n\n\n\nThis is similar to the steps above but does not depend on the query or database credentials. You are likely to need to use the Python requests library to get the data that will be used by sdv.\n\n\n\n\nWithout forcing this specification, then SDV treats columns as ‘categories’ and shuffles / re-arranges but the original data will appear.\nDefine fields that contain PII and need faking (see the sketchy documentation here and the Faker Documentation for a full list of providers. Here is a brief example that specifies Fakers for name and date of birth. Note that you must pass arguments to a faker as a list. NB: sdv also doesn’t always recognise the columns correctly. Here we specify data_of_birth explicitly as a date whilst working on the larger task of defining columns that contain PII. See field details\nExample specification to force faking for PII fields within the data\nfields = {\n    'dob': {\n        'type': 'datetime',\n        'format': '%Y-%m-%d',\n        'pii': True,\n        # the 'pii_category' key defines the Faker function name (method)\n        'pii_category': \"date_of_birth\",\n    },\n    'admission_age_years': {\n        'type': 'numerical',\n        'pii': True,\n        'pii_category': ['random_number', 2 ]\n    },\n    'name': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': 'name'\n    },\n    'mrn': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': ['random_number', 8 ]\n    },\n    'csn': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': ['numerify', '10########' ]\n    },\n    'postcode': {\n        'type': 'categorical',\n        'pii': True,\n        'pii_category': 'postcode',\n        'pii_locales': ['en_GB'],\n    },\n}"
  },
  {
    "objectID": "developer/deployment.html",
    "href": "developer/deployment.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "From the commandline of the GAE\ngit clone https://github.com/HYLODE/HyUi.git\ncd HyUi\ncp .env.example .env\n# Now hand edit the .env file with usernames/passwords\n# Set ENV=prod (rather than dev)\npytest # OPTIONAL\ndocker-compose up -d --build && docker-composes logs -f\nGo to http://my-host-name:8094/docs for the API. For example for this might be http://172.16.149.202:8094.\nGo to http://my-host-name:8095 for the dashboard landing page. For example for this might be http://172.16.149.202:8095."
  },
  {
    "objectID": "developer/environment-variables.html",
    "href": "developer/environment-variables.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "We use direnv to load environment variables, as these are only loaded when inside the project folder. This can prevent accidental conflicts with identically named variables.\n\n\nTo load the environment variables, first follow the direnv installation instructions, and make sure you have a .secrets file to store secrets and credentials. Then:\n\nOpen your terminal;\nNavigate to the project folder; and\n\nYou should see the following message:\ndirenv: error .envrc is blocked. Run `direnv allow` to approve its content.\n\nAllow direnv.\ndirenv allow\n\nYou only need to do this once, and again each time .envrc and .secrets are modified.\n\n\nThese instructions assume you are running on macOS with administrator privileges using a bash terminal. For other ways of installing direnv, and its shell hooks, consult the direnv documentation.\n\nOpen your terminal;\nInstall direnv using Homebrew;\nbrew install direnv\nAdd the shell hooks to your .bash_profile;\necho 'eval \"$(direnv hook bash)\"' >> ~/.bash_profile\nCheck that the shell hooks have been added correctly; and\ncat ~/.bash_profile\n\nThis should display eval \"$(direnv hook bash)\"\n\nRestart your terminal.\n\n\n\n\n\nSecrets and credentials must be stored in the .secrets file. This file is not version-controlled, so no secrets should be committed to GitHub.\nIn your terminal navigate to the root folder, and create a .secrets file.\ntouch .secrets\nOpen this new .secrets file using your preferred text editor, and add any secrets as environmental variables. For example, to add a JSON credentials file for Google BigQuery, save the following changes to .secrets.\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/credentials.json\"\nOnce complete, make sure the .secrets file has the following line uncommented out:\nsource_env \".secrets\"\nThis ensures direnv loads the .secrets file using .envrc without version-controlling .secrets."
  },
  {
    "objectID": "developer/setup.html",
    "href": "developer/setup.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Read this if\n\nyou are able to code in Python\nyou already have access to data at UCLH (e.g. EMAP databases or a HYLODE API)\nyou want to build a data visualisation using Plotly Dash\n\n\n\nThere are 2 primary environments needed for development: local machine and a live machine\n\n\nYour own laptop etc. without access to personally identifiable information (PII) etc. You wish to be able to build and run applications with test data.\n\n\n\nAn NHS machine or similar within sensitive environment with access to PII. You wish to be able to deploy the actual application or generate synthetic data to use in a local environment.\n\n\n\n\n\n\n\nPython 3.6.1+ installed\nInstall direnv, following these instructions\nInstall conda miniforge (e.g. brew install miniforge)\n\n\n\n\n\n\n\nNote\n\n\n\nWe assume that the majority of the development work will be on your own machine. You will therefore need to set-up your own dev environment. I have been using conda miniforge because this has the best compatibility with the ARM processor on the Mac. I think it should also work on other machines but I have not tested this.\n\n\n\n\n\ngit clone https://github.com/HYLODE/HyUi.git\ncd HyUi\ncp .env.example .env\n\n\n\nconda env create --file=./dev/environment.yml\nconda activate hyui\nThen confirm your set-up is OK\npytest -m smoke src/tests/smoke\n\n\n\nAutomatically activate your conda environment when you cd into the hyui directory\nhttps://github.com/theskumar/python-dotenv installed python-dotenv\npip install python-dotenv\nand https://direnv.net\nbrew install direnv\ntouch .envrc\ndirenv allow .\nadd the following to .zshrc\neval \"$(direnv hook zsh)\"\n\n\n\nsee docs/contributor_guide/pre_commit_hooks.md need to run this by hand the first time\npre-commit install\npre-commit run\n\n\n\nDepending on your dev computer, you may not want to run HyUI using Docker (e.g. Apple Silicon laptops experience very high memory use when running docker.)\n\n\nRun docker-compose up -d to stand up both backend and frontend services using docker.\nWARNING: you may need docker compose rather than docker-compose depending on your docker version WARNING: docker services depend on a valid .env file (specifically to hold the postgres credentials)\n\n\n\ncd ./src\nuvicorn api.main:app --reload --workers 4 --host 0.0.0.0 --port 8092\nthen navigate to http://localhost:8092/docs to view the API documentation\n\n\n\ncd ./src\nENV=dev DOCKER=False python apps/app.py\nthen navigate to http://localhost:8093 to view the app"
  },
  {
    "objectID": "developer/setup.html#development-workflow",
    "href": "developer/setup.html#development-workflow",
    "title": "HYLODE documentation",
    "section": "Development workflow",
    "text": "Development workflow\nOnce you’re up and running, you can review the development workflow in Development Workflow"
  },
  {
    "objectID": "developer/setup.html#apple-silicon-notes",
    "href": "developer/setup.html#apple-silicon-notes",
    "title": "HYLODE documentation",
    "section": "Apple Silicon notes",
    "text": "Apple Silicon notes\nSee Hylode HySys"
  },
  {
    "objectID": "developer/setup.html#overview",
    "href": "developer/setup.html#overview",
    "title": "HYLODE documentation",
    "section": "Overview",
    "text": "Overview\nThis is an experimental workflow designed to ultimately align the way packages are installed on development machines and in production. It is ultimately intended to replace Conda."
  },
  {
    "objectID": "developer/setup.html#architecture",
    "href": "developer/setup.html#architecture",
    "title": "HYLODE documentation",
    "section": "Architecture",
    "text": "Architecture\nThe root directory contains all the libraries that are used to power the HyUi dashboards.\n\napi: This is a FastAPI app that does all the data wrangling and accessing of external sources such as to databases and web services. For each route there should be an equivalent mock route that allows this app to serve mock data when run in development. For example, if there is a /census/ endpoint then there should be a corresponding /mock/census endpoint that only serves mock data.\nweb: This is contains the Plotly Dash web server code. It should only communicate with the api service. This allows the api service to provide mock data seamlessly when developing without needing to spin up other services to help development.\nmodels: This contains all the Pydantic models that are used to share data between the api and web.\n\n\nArchitecture Ideals\n\nweb should only communicate with api for data. This allows us to entirely mock data for web when developing.\nCommunication between web and api should only be done in a defined way using models described in models. models is the only shared code between api and web.\nPages in web/src/web/pages should be independent of each other so there is minimal/no sharing between two sets of pages. Common code can be put in a separate module or package in web."
  },
  {
    "objectID": "developer/setup.html#development",
    "href": "developer/setup.html#development",
    "title": "HYLODE documentation",
    "section": "Development",
    "text": "Development\n\nInstall Requirements\n\nInstall (https://github.com/pyenv/pyenv)[pyenv]\nInstall (https://github.com/pyenv/pyenv-virtualenv)[pyenv-virtualenv]\n\n\n\nCreate Required Virtual Environments\n> pyenv virtualenv 3.11.0 hyui-web\n> pyenv virtualenv 3.11.0 hyui-api\n\n\nActivate Virtual Environment\nIn one shell:\n> pyenv activate hyui-web\nIn another shell:\n> pyenv activate hyui-api\nThese will be the Python virtual environments that contains all the dependencies for the Dash web app and the API backend.\n\n\nInstall The Website App Requirements\nIn the hyui-web terminal in the root directory:\n> pip install -e \"utils[test]\"\n> pip install -e \"models[test]\"\n> pip install -e \"web[test]\"\n\n\nInstall The API Requirements\nIn the hyui-api terminal in the root directory:\n> pip install -e \"utils[test]\"\n> pip install -e \"models[test]\"\n> pip install -e \"api[test]\"\n\n\nConfiguring Environment Variables\nAn example environment variable file can be found at .env.example. The environment variables for each app are documented in the Pydantic Settings classes in web/src/web/config.py and api/src/api/config.py. An important one for development and production is the WEB_API_URL variable. This should be set to the host and port of your api server so most likely something like http://api:8000 for running in Docker or http://localhost:[port]/mock for development. Note that in development a /mock path has been added. This allows the Dash web app to use all the mock endpoints instead of the live endpoints.\n\n\nRunning The Servers Manually When Developing\nThe web server when in the hyui-web shell:\n> env $(grep -v '^#' .env | xargs) python -m web.main\nThe API server when in the hyui-api shell:\n> env $(grep -v '^#' .env | xargs) uvicorn api.main:app --reload --port 8092\nEnsure you replace .env with the correct path to your development .env file. The command env $(grep -v '^#' .env | xargs) loads the contents of the .env into the environment just for the current execution. The grep -v '^#' part excludes lines beginning with a # in the env file.\n\n\nRunning in PyCharm\nOn MacOS you will likely need to add the environment variable OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES to the web debugging process to stop a crash. This is due to a bug in the way the Dash background callback system works."
  },
  {
    "objectID": "developer/setup.html#deployment",
    "href": "developer/setup.html#deployment",
    "title": "HYLODE documentation",
    "section": "Deployment",
    "text": "Deployment\nFirst create a .env file and ensure it is correct.\n\nBuild the Docker Compose services.\nIn the code directory:\n> docker compose build\n\n\nRun the Docker Compose services\nIn the code directory:\n> docker compose up"
  },
  {
    "objectID": "developer/workflow.html",
    "href": "developer/workflow.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Note\n\n\n\n\nPersona\n\ndeveloper reading this who wants to deploy an dash app\n\nSetting\n\nworking on their own laptop with access to UCLH data and machines"
  },
  {
    "objectID": "developer/workflow.html#make-synthetic-version-of-the-data",
    "href": "developer/workflow.html#make-synthetic-version-of-the-data",
    "title": "HYLODE documentation",
    "section": "1. Make synthetic version of the data",
    "text": "1. Make synthetic version of the data\nWe imagine that the developer has the appropriate permissions to view the raw data including patient identifiable information (either themselves, or in partnership with a colleague). A justification for this position is [here][provisioning]. Practically, this means early interactive data exploration using the UCLH Datascience Desktop and similar tools, and in turn access to EMAP and Clarity/Caboodle.\nThis should generate an initial data specification, and this can be used to generate synthetic data. The synthetic data can then be used to drive the rest of the pathway."
  },
  {
    "objectID": "developer/workflow.html#develop-with-synthetic-data",
    "href": "developer/workflow.html#develop-with-synthetic-data",
    "title": "HYLODE documentation",
    "section": "2. Develop with synthetic data",
    "text": "2. Develop with synthetic data"
  },
  {
    "objectID": "developer/workflow.html#write-some-tests-and-quality-control",
    "href": "developer/workflow.html#write-some-tests-and-quality-control",
    "title": "HYLODE documentation",
    "section": "3. Write some tests and quality control",
    "text": "3. Write some tests and quality control"
  },
  {
    "objectID": "developer/workflow.html#update-the-plot-to-run-live",
    "href": "developer/workflow.html#update-the-plot-to-run-live",
    "title": "HYLODE documentation",
    "section": "4. Update the plot to run live",
    "text": "4. Update the plot to run live"
  },
  {
    "objectID": "developer/workflow.html#allow-inspection-over-the-last-months",
    "href": "developer/workflow.html#allow-inspection-over-the-last-months",
    "title": "HYLODE documentation",
    "section": "5. Allow inspection over the last months",
    "text": "5. Allow inspection over the last months"
  },
  {
    "objectID": "developer/workflow.html#split-by-specialty",
    "href": "developer/workflow.html#split-by-specialty",
    "title": "HYLODE documentation",
    "section": "6. Split by specialty",
    "text": "6. Split by specialty"
  },
  {
    "objectID": "developer/workflow.html#prepare-synthetic-data-at-uclh",
    "href": "developer/workflow.html#prepare-synthetic-data-at-uclh",
    "title": "HYLODE documentation",
    "section": "Prepare synthetic data at UCLH",
    "text": "Prepare synthetic data at UCLH\n\nLog on to a UCLH machine\nOpen a browser (e.g. Chrome)\nLog on to the HyUi JupyterLab instance (e.g. http://uclvlddpragae07:8091/)\nEither\n\nIf you’re working with a SQL script then follow the steps in synth_test_data_consults.ipynb\nIf you’re working with a local HTTP endpoint (e.g. HYLODE API for the census http://uclvlddpragae07:5006/emap/icu/census/T03/) follow the steps in synth_test_data_census.ipynb\n\nCopy out your synthetic data\n\nSee the Synthetic Data Preparation docs."
  },
  {
    "objectID": "developer/workflow.html#set-up-local-api",
    "href": "developer/workflow.html#set-up-local-api",
    "title": "HYLODE documentation",
    "section": "Set-up local API",
    "text": "Set-up local API\nmake a local API module create the __init__.py with reference to routes the directory should contain\napi/mymodule\n|\n|-- __init__.py         // define project wide variables\n|-- mock.h5             // synthetic data create above for mocking\n|-- mock.sql            // query to run against mock database\n|-- model.py            // pydantic (SQLmodel) to define data\nadd a new route to config.settings.ModuleNames\nclass ModuleNames(str, Enum):\n    \"\"\"\n    Class that defines routes for each module\n    \"\"\"\n\n    consults = \"consults\"\n    sitrep = \"sitrep\"\n    mymodule = \"mymodule\""
  },
  {
    "objectID": "developer/workflow.html#set-up-local-testing",
    "href": "developer/workflow.html#set-up-local-testing",
    "title": "HYLODE documentation",
    "section": "Set-up local testing",
    "text": "Set-up local testing\nthen run mock.py (and this will insert the mock data into the local sqlite database) then either make api or uvicorn api.main:app --reload --workers 4 --host 0.0.0.0 --port 8092 navigate to http://localhost:8092/docs to check that it works\nthen create a module for testing (just duplicate an existing one and adapt) you will need a local conftest.py to set up the mockdata"
  },
  {
    "objectID": "developer/workflow.html#set-up-local-dash-app",
    "href": "developer/workflow.html#set-up-local-dash-app",
    "title": "HYLODE documentation",
    "section": "Set-up local Dash app",
    "text": "Set-up local Dash app\nthen create an app in src/apps/mymodule ideally import headers and other attributes (from src/apps/index.py) for consistent styling register the new routes in src/apps/index.py"
  },
  {
    "objectID": "developer/workflow.html#debugging-after-deployment-in-the-live-environment",
    "href": "developer/workflow.html#debugging-after-deployment-in-the-live-environment",
    "title": "HYLODE documentation",
    "section": "Debugging after deployment in the live environment",
    "text": "Debugging after deployment in the live environment\nSince we don’t have root access, then the deployment happens via docker. This means that the debugging must also happen inside a live docker container. The following seems to work.\nEnsure that your docker-compose file includes these options for the image that you wish to debug.\nIf this is your docker-compose config for the api service:\n\n  api:\n    build:\n      context: ./src\n      dockerfile: Dockerfile.api\n...\nthen add these lines\n    stdin_open: true\n    tty: true\n\nRun docker-compose up -d --build as usual.\nFind the ID of the approprite container via docker container ps | grep hyui (e.g. abc123)\nUse that ID to run docker attach abc123. Don’t be put off if nothing seems to happen.\nWhenever the python debugger is triggered (e.g. via a breakpoint import pdb; pdb.set_trace()) the container will return control to the terminal above.\nTo exit without killing the container then use CONTROL-P, CONTROL-Q instead of CONTROL-C\n\nAll this via https://blog.lucasferreira.org/howto/2017/06/03/running-pdb-with-docker-and-gunicorn.html with tips for the Python Debugger via https://realpython.com/python-debugging-pdb/."
  },
  {
    "objectID": "notes/provisioning.html",
    "href": "notes/provisioning.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Footnotes\n\n\nDifficulties in deployment are also the Achilles heel of a Trusted Research Environment. Whilst safe, these walled gardens mean that models are developed offline without the benefit of clinical testing.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "vignettes/vignettes.html",
    "href": "vignettes/vignettes.html",
    "title": "Vignettes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nWorking notes on merging Caboodle and EMAP (NHNN)\n\n\n\n\n\n\nWorking notes on merging caboodle info on beds with EMAP (UCLH)\n\n\n\n\n\n\nICU aggregate probabilities\n\n\n\n\n\n\nMake predictions using a saved binomial model for ICU elective admission\n\n\n\n\n\n\nMake predictions using a saved poisson model for ICU non-elective admission\n\n\n\n\n\n\nWrangle live beds into a simple census\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "vignettes/icu-tap-aggregate-probs.html",
    "href": "vignettes/icu-tap-aggregate-probs.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "import json\nfrom datetime import date\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nfrom scipy.stats import binom\n\ndata = json.dumps(\n    {\n        \"horizon_dt\": date.today().strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\n        \"department\": \"gwb\",\n    }\n)\n\n\nelective_preds = requests.post(\"http://uclvlddpragae08:5219/predict/\", data).json()\nnonelective_preds = requests.post(\"http://uclvlddpragae08:5230/predict/\", data).json()\n\n\nelective = []\nfor bed_num in elective_preds[\"data\"]:\n    elective.append(bed_num[\"probability\"])\nprint(\n    \"If the number of elective patients on list is \" + str(len(elective) - 1),\n    \"the ML model will return a set of predictions from 0 beds, needed, up to that maximum, so our first array has \"\n    + str(len(elective))\n    + \" values, starting with zero\",\n)\nposs_beds = list(range(0, len(elective)))\nprint(poss_beds)\nfig, ax = plt.subplots(1, 1)\nplt.scatter(poss_beds, elective)\nplt.title(\"Distribution of elective probabilities\")\nplt.show()\n\n\nnonelective = []\nfor bed_num in nonelective_preds[\"data\"]:\n    nonelective.append(bed_num[\"probability\"])\nprint(\n    \"The ML model will return predictions for a set of beds needed for nonelective patients that might arrive ranging from 0 to  \"\n    + str(len(nonelective) - 1),\n    \" so our second array has \" + str(len(nonelective)) + \" values, starting with zero\",\n)\nposs_beds = list(range(0, len(nonelective)))\nprint(poss_beds)\n\n\nfig, ax = plt.subplots(1, 1)\nplt.scatter(poss_beds, nonelective)\nplt.title(\"Distribution of nonelective probabilities\")\nplt.show()\n\n\nnum_on_unit = 18\nprob_still_here = 0.8\nonunit = binom.pmf(list(range(0, num_on_unit + 1)), num_on_unit, prob_still_here)\nprint(\n    \"Let's say there are \"\n    + str(num_on_unit)\n    + \" already on the unit each with a probability of \"\n    + str(prob_still_here)\n    + \" of still being here in 24 hours time, so our third array has \"\n    + str(len(onunit))\n    + \" values, starting with zero\"\n)\nposs_beds = list(range(0, len(onunit)))\nprint(poss_beds)\n\n\nfig, ax = plt.subplots(1, 1)\nplt.scatter(poss_beds, onunit)\nplt.title(\"Distribution of probabilities for pats on unit\")\nplt.show()\n\n\nprint(\n    \"The total possible number of admissions is the sum of the maximum possible from each of these separate distributions, which is \"\n    + str(num_on_unit + len(elective) - 1 + len(nonelective) - 1),\n    \"so the length of our aggregated predictions is one more than this number, as it is starting at zero\",\n)\ntot_beds_array = list(range(0, len(onunit) + len(elective) - 1 + len(nonelective) - 1))\nprint(tot_beds_array)\n\n\nprint(\n    \"To get a probability distribution over all these, we simply convolve them together like this: \"\n)\naggregated = np.convolve(onunit, np.convolve(elective, nonelective))\n\n\nfig, ax = plt.subplots(1, 1)\nplt.scatter(tot_beds_array, aggregated)\nplt.title(\"Distribution of aggregated probabilities\")\nplt.show()"
  },
  {
    "objectID": "vignettes/wrangle_beds.html",
    "href": "vignettes/wrangle_beds.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Run the beds live sql and then wrangle the data into a simple census\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport sqlalchemy as sa\n\nAssume that we’re running from the notebooks directory and need to pull data from ./src/mock/mock.db\n\nsqlite_db = \"../src/mock/mock.db\"\nassert Path(sqlite_db).is_file()\n\n\nengine = sa.create_engine(f\"sqlite:///{sqlite_db}\")\n\n\ndf = pd.read_sql(\"bedsmock\", engine)\n\n\ndf.iloc[0]\n\nSo now you want to return a dataframe with the following characteristics - per department (one row per department) - count number of beds - count number of occupied beds - count number of empty beds - last date of closed discharge from that department (i.e. if > 1 week and most beds empty then is the department closed) - number of side rooms?\n\ntemp = (\n    df[\"location_string\"]\n    .str.split(\"^\", expand=True)\n    .rename(columns={0: \"dept\", 1: \"room\", 2: \"bed\"})\n)\nfor s in [\"dept\", \"room\", \"bed\"]:\n    df[s] = temp[s]\n\n\ndel temp\n\nremove null and waiting locations\n\nmask = df[\"bed\"].str.lower().isin([\"null\", \"wait\"])\ndf = df[~mask]\n\n\ndf.shape\n\n\ngroups = df.groupby(\"department\")\n\n\ngroups.get_group(\"GWB L01 CRITICAL CARE\")\n\n\nres = groups.agg(\n    beds=(\"location_id\", \"count\"),\n    patients=(\"occupied\", \"sum\"),\n    last_dc=(\"cvl_discharge\", lambda x: x.max(skipna=True)),\n    modified_at=(\"modified_at\", \"max\"),\n)\nres[\"empties\"] = res[\"beds\"] - res[\"patients\"]\nres[\"opens\"] = res[\"empties\"]  # place holder : need to subtract closed from empties\n\n\nres[\"last_dc\"] = (res[\"modified_at\"] - res[\"last_dc\"]).apply(\n    lambda x: pd.Timedelta.floor(x, \"d\")\n)\n\n\nres[\"closed_temp\"] = pd.DataFrame(\n    [\n        res[\"last_dc\"] > pd.Timedelta(2, \"days\"),\n        res[\"last_dc\"] <= pd.Timedelta(30, \"days\"),\n        res[\"patients\"] == 0,\n    ]\n).T.all(axis=\"columns\")\n\nres[\"closed_perm\"] = pd.DataFrame(\n    [\n        res[\"last_dc\"] > pd.Timedelta(30, \"days\"),\n        res[\"patients\"] == 0,\n    ]\n).T.all(axis=\"columns\")\n\n\nmask = ~res[\"closed_perm\"]\n\nres = res[mask]\nres = res[\n    [\"beds\", \"patients\", \"empties\", \"opens\", \"last_dc\", \"closed_temp\" \"modified_at\"]\n]\n\n\nres\n\n\nfrom typing import List\n\nimport pandas as pd\n\n\ndef _split_location_string(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Splits a location string into dept/room/bed\n    \"\"\"\n    temp = (\n        df[\"location_string\"]\n        .str.split(\"^\", expand=True)\n        .rename(columns={0: \"dept\", 1: \"room\", 2: \"bed\"})\n    )\n    for s in [\"dept\", \"room\", \"bed\"]:\n        df[s] = temp[s]\n    return df\n\n\ndef _remove_non_beds(\n    df: pd.DataFrame, nonbeds: List[str] = [\"null\", \"wait\"]\n) -> pd.DataFrame:\n    \"\"\"\n    Removes non beds e.g. null, wait\n    \"\"\"\n    mask = df[\"bed\"].str.lower().isin(nonbeds)\n    df = df[~mask]\n    return df\n\n\ndef _aggregate_by_department(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Aggregation from location (bed) level to ward level\n    \"\"\"\n    groups = df.groupby(\"department\")\n    # aggregate by dept\n    res = groups.agg(\n        beds=(\"location_id\", \"count\"),\n        patients=(\"occupied\", \"sum\"),\n        last_dc=(\"cvl_discharge\", lambda x: x.max(skipna=True)),\n        modified_at=(\"modified_at\", \"max\"),\n    )\n    # calculate additional numbers\n    res[\"empties\"] = res[\"beds\"] - res[\"patients\"]\n    res[\"opens\"] = res[\"empties\"]  # place holder : need to subtract closed from empties\n    res[\"last_dc\"] = (\n        (res[\"modified_at\"] - res[\"last_dc\"])\n        .apply(lambda x: pd.Timedelta.floor(x, \"d\"))\n        .dt.days\n    )\n\n    # defined closed: temp and perm\n    res[\"closed_temp\"] = pd.DataFrame(\n        [\n            res[\"last_dc\"] > 2,\n            res[\"last_dc\"] <= 30,\n            res[\"patients\"] == 0,\n        ]\n    ).T.all(axis=\"columns\")\n\n    res[\"closed_perm\"] = pd.DataFrame(\n        [\n            res[\"last_dc\"] > 30,\n            res[\"patients\"] == 0,\n        ]\n    ).T.all(axis=\"columns\")\n\n    # drop closed perm\n    mask = ~res[\"closed_perm\"]\n\n    res = res[mask]\n    res = res[\n        [\n            \"beds\",\n            \"patients\",\n            \"empties\",\n            \"opens\",\n            \"last_dc\",\n            \"closed_temp\",\n            \"modified_at\",\n        ]\n    ]\n    res.reset_index(inplace=True)\n    return res\n\n\ndef aggregate_by_department(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Aggregation from location (bed) level to ward level\n    Wrapper function\n    \"\"\"\n    df = _split_location_string(df)\n    df = _remove_non_beds(df)\n    df = _aggregate_by_department(df)\n    return df\n\n\ndf = pd.read_sql(\"bedsmock\", engine)\naggregate_by_department(df)"
  },
  {
    "objectID": "vignettes/beds_v3UCLH.html",
    "href": "vignettes/beds_v3UCLH.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "2022-07-03\nworking notes on merging caboodle info on beds with EMAP\naim is to build a reliable view of bed census\nprogrammatically returns a list of locatoins with Caboodle detail\nbut no testing/quality control\nworks in conjunction with add_caboodle2emap_beds.py\nuse to populate base bed definitions for the tower flow"
  },
  {
    "objectID": "vignettes/beds_v3UCLH.html#set-up-incl-database-connections",
    "href": "vignettes/beds_v3UCLH.html#set-up-incl-database-connections",
    "title": "HYLODE documentation",
    "section": "Set-up incl database connections",
    "text": "Set-up incl database connections\n\nimport os\nimport urllib\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\n\n\ndef emap_db() -> Engine:\n    url = \"postgresql+psycopg2://{}:{}@{}:{}/{}\".format(\n        os.getenv(\"EMAP_DB_USER\"),\n        os.getenv(\"EMAP_DB_PASSWORD\"),\n        os.getenv(\"EMAP_DB_HOST\"),\n        os.getenv(\"EMAP_DB_PORT\"),\n        os.getenv(\"EMAP_DB_NAME\"),\n    )\n    engine = create_engine(\n        url, pool_size=4, max_overflow=6, connect_args={\"connect_timeout\": 120}\n    )\n    return engine\n\n\ndef caboodle_db() -> Engine:\n    db_host = os.getenv(\"CABOODLE_DB_HOST\")\n    db_user = os.getenv(\"CABOODLE_DB_USER\")\n    db_password = os.getenv(\"CABOODLE_DB_PASSWORD\")\n    db_port = os.getenv(\"CABOODLE_DB_PORT\")\n    db_name = os.getenv(\"CABOODLE_DB_NAME\")\n    connection_str = f\"mssql+pyodbc://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server\"\n    engine = create_engine(connection_str)\n    return engine\n\n\nemap_engine = emap_db()\ncaboodle_engine = caboodle_db()"
  },
  {
    "objectID": "vignettes/beds_v3UCLH.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "href": "vignettes/beds_v3UCLH.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "title": "HYLODE documentation",
    "section": "Reliable way of joining Caboodle and EMAP bed level data",
    "text": "Reliable way of joining Caboodle and EMAP bed level data\nFirst load EMAP location table into memory\nDo this with no modifications / transformations other than the join\n\nq = \"\"\"\nSELECT \n    lo.location_id, lo.location_string, lo.department_id, lo.room_id, lo.bed_id, dept.name department, dept.speciality, room.name room\nFROM star.location lo\nLEFT JOIN star.department dept ON lo.department_id = dept.department_id\nLEFT JOIN star.room ON lo.room_id = room.room_id\n\"\"\"\ndfe = pd.read_sql_query(q, emap_engine)\ndfe.head()\n\nAnd load Caboodle DepartmentDim into memory\n\nq = \"\"\"\nSELECT \n DepartmentKey\n,BedEpicId\n,Name\n,DepartmentName\n,RoomName\n,BedName\n,IsBed\n,BedInCensus\n,IsDepartment\n,IsRoom\n,IsCareArea\n,DepartmentExternalName\n,DepartmentSpecialty\n,DepartmentType\n,DepartmentServiceGrouper\n,DepartmentLevelOfCareGrouper\n,LocationName\n,ParentLocationName\n,_CreationInstant\n,_LastUpdatedInstant\nFROM dbo.DepartmentDim\n\"\"\"\ndfc = pd.read_sql_query(q, caboodle_engine)\ndfc.head()\n\nNow join these two tables - don’t attempt joins where there is no room/bed level data (b/c they’re not physical locations - you need a multi-key join - EMAP ‘name’ (department.name) joins on to Caboodle ‘DepartmentName’ - EMAP ‘bed’ (derived by splitting location.location_string) joins on to Caboodle ‘Name’ - drop ‘wait’ beds since these duplicate and block a one-to-one merge - try to be rigorous in pd.merge - indicator=True to allow inspection post merge - validate='one_to_one' to throw an error if duplicates found\nNote - sometimes (in Caboodle) DepartmentName and Name are duplicated so pick the most recently ‘created’\nusing …\nNow load external code\n\nimport importlib\n\nimport add_caboodle2emap_beds\n\nimportlib.reload(add_caboodle2emap_beds)\nfrom add_caboodle2emap_beds import bed_merge\n\n\ndepartments = [\n    # Built from Tower Report 14 Jun 2022\n    # NAME                         # n emap locations\n    \"UCH T01 ACUTE MEDICAL\",  # 86\n    \"UCH T01 ENHANCED CARE\",  # 20\n    \"UCH T03 INTENSIVE CARE\",  # 37\n    \"UCH T06 HEAD (T06H)\",  # 27\n    \"UCH T06 CENTRAL (T06C)\",  # 25\n    \"UCH T06 SOUTH PACU\",  # 22\n    \"UCH T06 GYNAE (T06G)\",  # 18\n    \"UCH T07 NORTH (T07N)\",  # 45\n    \"UCH T07 CV SURGE\",  # 37\n    \"UCH T07 SOUTH\",  # 33\n    \"UCH T07 SOUTH (T07S)\",  # 23\n    \"UCH T07 HDRU\",  # 20\n    \"UCH T08 NORTH (T08N)\",  # 28\n    \"UCH T08 SOUTH (T08S)\",  # 25\n    \"UCH T08S ARCU\",  #  6\n    \"UCH T09 SOUTH (T09S)\",  # 34\n    \"UCH T09 NORTH (T09N)\",  # 32\n    \"UCH T09 CENTRAL (T09C)\",  # 25\n    \"UCH T10 SOUTH (T10S)\",  # 34\n    \"UCH T10 NORTH (T10N)\",  # 32\n    \"UCH T10 MED (T10M)\",  # 16\n    \"UCH T11 SOUTH (T11S)\",  # 27\n    \"UCH T11 NORTH (T11N)\",  # 25\n    \"UCH T11 EAST (T11E)\",  # 16\n    \"UCH T11 NORTH (T11NO)\",  #  8\n    \"UCH T12 SOUTH (T12S)\",  # 32\n    \"UCH T12 NORTH (T12N)\",  # 23\n    \"UCH T13 SOUTH (T13S)\",  # 31\n    \"UCH T13 NORTH ONCOLOGY\",  # 26\n    \"UCH T13 NORTH (T13N)\",  # 26\n    \"UCH T14 NORTH TRAUMA\",  # 28\n    \"UCH T14 NORTH (T14N)\",  # 28\n    \"UCH T14 SOUTH ASU\",  # 22\n    \"UCH T14 SOUTH (T14S)\",  # 17\n    \"UCH T15 SOUTH DECANT\",  # 21\n    \"UCH T15 SOUTH (T15S)\",  # 21\n    \"UCH T15 NORTH (T15N)\",  # 16\n    \"UCH T15 NORTH DECANT\",  # 15\n    \"UCH T16 NORTH (T16N)\",  # 19\n    \"UCH T16 SOUTH (T16S)\",  # 18\n    \"UCH T16 SOUTH WINTER\",  # 17\n    \"GWB L01 ELECTIVE SURG\",  # 37\n    \"GWB L01 CRITICAL CARE\",  # 12\n    \"GWB L02 NORTH (L02N)\",  # 19\n    \"GWB L02 EAST (L02E)\",  # 19\n    \"GWB L03 NORTH (L03N)\",  # 19\n    \"GWB L03 EAST (L03E)\",  # 19\n    \"GWB L04 NORTH (L04N)\",  # 20\n    \"GWB L04 EAST (L04E)\",  # 17\n    \"WMS W04 WARD\",  # 28\n    \"WMS W03 WARD\",  # 27\n    \"WMS W02 SHORT STAY\",  # 20\n    \"WMS W01 CRITICAL CARE\",  # 11\n]\n\n\ndepartments = [\n    # Built from Tower Report 14 Jun 2022\n    # NAME                         # n emap locations\n    \"UCH T01 ACUTE MEDICAL\",  # 86\n    \"UCH T01 ENHANCED CARE\",  # 20\n    \"UCH T03 INTENSIVE CARE\",  # 37\n    \"UCH T06 HEAD (T06H)\",  # 27\n    \"UCH T06 CENTRAL (T06C)\",  # 25\n    \"UCH T06 SOUTH PACU\",  # 22\n    \"UCH T06 GYNAE (T06G)\",  # 18\n    \"UCH T07 NORTH (T07N)\",  # 45\n    \"UCH T07 CV SURGE\",  # 37\n    \"UCH T07 SOUTH\",  # 33\n    \"UCH T07 SOUTH (T07S)\",  # 23\n    \"UCH T07 HDRU\",  # 20\n    \"UCH T08 NORTH (T08N)\",  # 28\n    \"UCH T08 SOUTH (T08S)\",  # 25\n    \"UCH T08S ARCU\",  #  6\n    \"UCH T09 SOUTH (T09S)\",  # 34\n    \"UCH T09 NORTH (T09N)\",  # 32\n    \"UCH T09 CENTRAL (T09C)\",  # 25\n    \"UCH T10 SOUTH (T10S)\",  # 34\n    \"UCH T10 NORTH (T10N)\",  # 32\n    \"UCH T10 MED (T10M)\",  # 16\n    \"UCH T11 SOUTH (T11S)\",  # 27\n    \"UCH T11 NORTH (T11N)\",  # 25\n    \"UCH T11 EAST (T11E)\",  # 16\n    \"UCH T11 NORTH (T11NO)\",  #  8\n    \"UCH T12 SOUTH (T12S)\",  # 32\n    \"UCH T12 NORTH (T12N)\",  # 23\n    \"UCH T13 SOUTH (T13S)\",  # 31\n    \"UCH T13 NORTH ONCOLOGY\",  # 26\n    \"UCH T13 NORTH (T13N)\",  # 26\n    \"UCH T14 NORTH TRAUMA\",  # 28\n    \"UCH T14 NORTH (T14N)\",  # 28\n    \"UCH T14 SOUTH ASU\",  # 22\n    \"UCH T14 SOUTH (T14S)\",  # 17\n    \"UCH T15 SOUTH DECANT\",  # 21\n    \"UCH T15 SOUTH (T15S)\",  # 21\n    \"UCH T15 NORTH (T15N)\",  # 16\n    \"UCH T15 NORTH DECANT\",  # 15\n    \"UCH T16 NORTH (T16N)\",  # 19\n    \"UCH T16 SOUTH (T16S)\",  # 18\n    \"UCH T16 SOUTH WINTER\",  # 17\n    \"GWB L01 ELECTIVE SURG\",  # 37\n    \"GWB L01 CRITICAL CARE\",  # 12\n    \"GWB L02 NORTH (L02N)\",  # 19\n    \"GWB L02 EAST (L02E)\",  # 19\n    \"GWB L03 NORTH (L03N)\",  # 19\n    \"GWB L03 EAST (L03E)\",  # 19\n    \"GWB L04 NORTH (L04N)\",  # 20\n    \"GWB L04 EAST (L04E)\",  # 17\n    \"WMS W04 WARD\",  # 28\n    \"WMS W03 WARD\",  # 27\n    \"WMS W02 SHORT STAY\",  # 20\n    \"WMS W01 CRITICAL CARE\",  # 11\n]\n\n\ndfm = bed_merge(df_emap=dfe, df_caboodle=dfc, departments=departments)\ndfm.head()\n\n\ndfm._merge.value_counts()\n\n\ndfm.to_csv(\"beds.tsv\", sep=\"\\t\", index_label=\"local_id\")"
  },
  {
    "objectID": "vignettes/icu-tap-elective-predictions.html",
    "href": "vignettes/icu-tap-elective-predictions.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "This notebook shows how to use the models I have saved to ML flow, for the purposes of predicting how many elective patients will arrive in the ICU in the 24 hours following 9.15 on a given day.\nOne of three locations can be requested: tower, gwb and wms (all lower case). These have different trained models. You need to retrieve the relevant model from ML flow for the location requested. You would do this be setting model_name and model_version to the saved values (as an example for the tower: MODEL__TAP_ELECTIVE_TOWER__NAME, MODEL__TAP_ELECTIVE_TOWER__VERSION need to be set; these should eventually be saved as constants in global settings)\nLogic: - Find out how many patients are on the surgical list for a given day - Retrieve a ML model which uses simple date parameters (but could one day be more sophisticated) to determine probability of ICU admission for any patient (not taking patient level characteristics into account) based on day of week - Use this probability to generate a binomial probability distribution over the number of beds needed in ICU for those patients\nNOTES - we decided not to make predictions for weekends, as almost all weekend days have no elective patients - we use a lens here which does not really serve any purpose, other than to one-hot encode the day of week. However, in future, other covariates (eg the number of patients in the hospital currently) could be added to the model, and the lens is already in place to add scaling functions\n\n\nThe input data to the models takes the form of of one-row dataframe with the following columns; [‘model_function’, ‘date’, ‘icu_counts’, ‘noticu_counts’, ‘wkday’, ‘N’] - model_function - set this to ‘binom’ - date - use pd.to_datetime to set format eg pd.to_datetime(‘2022-08-08’) - icu_counts - set this field to zero [this is an artefact of the lens method] - noticu_counts - ditto - wkday - an integer for the day of the week, where Monday is 0 - N - number of patients in the elective list (retrieved from Caboodle)\n\n\n\nThe called function SKProbabilityPredictorStats() will return an error if: - you request a weekend day - there are no patients on the elective list\n\n\n\n\nimport pkg_resources\n\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([f\"{i.key}=={i.version}\" for i in installed_packages])\n\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\n\nimport os\nimport pickle\nimport tempfile\nfrom pathlib import Path\n\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\n\nimport urllib\n\nfrom hylib import settings\nfrom hylib.dt import LONDON_TZ\nfrom hymind.predict.base import BaseLensPredictor\nfrom patsy import dmatrices\nfrom scipy.stats import binom\nfrom sqlalchemy import create_engine\n\n\n\n\n\nconn_str = \"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={},{};DATABASE={};UID={};PWD={}\".format(\n    settings.CABOODLE_DB_HOST,\n    settings.CABOODLE_DB_PORT,\n    settings.CABOODLE_DB_NAME,\n    settings.CABOODLE_DB_USER,\n    settings.CABOODLE_DB_PASSWORD,\n)\ncaboodle_db = create_engine(\n    f\"mssql+pyodbc:///?odbc_connect={urllib.parse.quote_plus(conn_str)}\"\n)\n\n\n\n\n\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n\nmlflow_var = os.getenv(\"HYMIND_REPO_TRACKING_URI\")\nmlflow.set_tracking_uri(mlflow_var)\n\nclient = MlflowClient()\n\n\nMODEL__TAP_ELECTIVE_TOWER__NAME = \"tap_elective_tower\"\nMODEL__TAP_ELECTIVE_TOWER__VERSION = 5\nMODEL__TAP_ELECTIVE_GWB__NAME = \"tap_elective_gwb\"\nMODEL__TAP_ELECTIVE_GWB__VERSION = 5\nMODEL__TAP_ELECTIVE_WMS__NAME = \"tap_elective_wms\"\nMODEL__TAP_ELECTIVE_WMS__VERSION = 5\n\n\ndef get_model_details(location):\n\n    if location == \"tower\":\n        model_name, model_version = (\n            MODEL__TAP_ELECTIVE_TOWER__NAME,\n            MODEL__TAP_ELECTIVE_TOWER__VERSION,\n        )\n    elif location == \"gwb\":\n        model_name, model_version = (\n            MODEL__TAP_ELECTIVE_GWB__NAME,\n            MODEL__TAP_ELECTIVE_GWB__VERSION,\n        )\n    else:\n        model_name, model_version = (\n            MODEL__TAP_ELECTIVE_WMS__NAME,\n            MODEL__TAP_ELECTIVE_WMS__VERSION,\n        )\n    return model_name, model_version\n\n\n\n\n\nclass SKProbabilityPredictorStats(BaseLensPredictor):\n    def __init__(self, model_name: str, model_version: int) -> None:\n        super().__init__(model_name, model_version)\n        self.model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n        self.expr = self._load_expr(self.model_info.run_id)\n        self.lens = self._load_lens(self.model_info.run_id)\n        self.input_df = self._is_weekday(input_df)\n        self.input_df = self._elective_list_gt0(input_df)\n\n    def _is_weekday(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if not input_df.iloc[0, 4] in list(range(0, 5)):\n                raise ValueError(\"Date requested is not a weekday\")\n            return input_df\n\n    def _elective_list_gt0(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if input_df.iloc[0, 5] == 0:\n                raise ValueError(\"There are no patients on the elective list\")\n            return input_df\n\n    @staticmethod\n    def _load_expr(run_id: str):\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp_dir = Path(tmp)\n\n            client.download_artifacts(run_id, \"expr\", tmp_dir)\n\n            expr_path = next((tmp_dir / \"expr\").rglob(\"*.txt\"))\n            with open(expr_path, \"rb\") as f:\n                expr = f.read()\n            expr = str(expr, \"utf-8\")\n\n            return expr\n\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n\n        X_df = self.lens.transform(input_df)\n        X_df__, X_df = dmatrices(self.expr, X_df, return_type=\"dataframe\")\n\n        predictions_set_df = self.model.get_prediction(X_df)\n        p = predictions_set_df.summary_frame().iloc[0, 0]\n\n        if input_df.iloc[0, 0] == \"binom\":\n\n            N = input_df.iloc[0, 5]\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": binom.pmf(list(range(0, N + 1)), N, p),\n                }\n            )\n\n        else:\n\n            N = 11\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": poisson.pmf(list(range(0, N + 1)), p),\n                }\n            )\n\n        predictions_df[\"predict_dt\"] = datetime.now(LONDON_TZ)\n        predictions_df[\"model_name\"] = self.model_name\n        predictions_df[\"model_version\"] = self.model_version\n        predictions_df[\"run_id\"] = self.model_info.run_id\n\n        return predictions_df\n\n\n\n\n\ndef get_elective_cases(date, location):\n\n    if location == \"tower\":\n\n        department1 = \"UCH P03 THEATRE SUITE\"\n        department2 = \"UCH T02 DAY SURG THR\"\n\n    elif location == \"wms\":\n\n        department1 = \"WMS W01 THEATRE SUITE\"\n        department2 = \"WMS W01 THEATRE SUITE\"\n\n    elif location == \"gwb\":\n\n        department1 = \"GWB B-1 THEATRE SUITE\"\n        department2 = \"GWB B-1 THEATRE SUITE\"\n\n    data = pd.read_sql(\n        \"\"\"\n        SELECT COUNT (DISTINCT scf.[PatientKey]) \n  \n      FROM [CABOODLE_REPORT].[dbo].[SurgicalCaseFact] scf \n      LEFT JOIN [CABOODLE_REPORT].[dbo].[WaitingListEntryFact] wlef ON wlef.[SurgicalCaseKey] = scf.[SurgicalCaseKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[SurgicalCaseUclhFactX] scufx ON scf.[SurgicalCaseKey] = scufx.[SurgicalCaseKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[PatientDim] patd ON scf.[PatientDurableKey] = patd.[DurableKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[ProcedureDim] pd ON scf.[PrimaryProcedureKey] = pd.[ProcedureKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DepartmentDim] dd ON scf.[OperatingRoomKey] = dd.[DepartmentKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datewl ON wlef.[PlacedOnWaitingListDateKey] = datewl.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datedta ON wlef.[DecidedToAdmitDateKey] = datedta.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datesurg ON scf.[SurgeryDateKey] = datesurg.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datecasereq ON scf.[CaseRequestDateKey] = datecasereq.[DateKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[TimeOfDayDim] todcase ON scf.[CaseRequestTimeOfDayKey] = todcase.[TimeOfDayKey]\n      LEFT JOIN [CABOODLE_REPORT].[dbo].[DateDim] datecancel ON scufx.[CancelDateKey] = datecancel.[DateKey]\n      WHERE scf.[PatientDurableKey] > 1 AND scf.[PatientDurableKey] IS NOT NULL\n    --  AND wlef.[SurgicalService] != '*Unspecified' \n      AND scf.[PrimaryService] != 'Obstetrics' AND scf.[PrimaryService] != 'Neurosurgery' AND scf.[PrimaryService] != 'Paediatric Dental'\n      AND scf.[PatientInFacilityDateKey] < 0\n      AND dd.[DepartmentName] != 'NHNN THEATRE SUITE' AND dd.[DepartmentName] != 'RNTNE THEATRE SUITE' AND dd.[DepartmentName] != 'EGA E02 LABOUR WARD'\n      AND dd.[DepartmentName] != 'MCC H-1 THEATRE SUITE' AND dd.[DepartmentName] != 'UCH ANAESTHESIA DEPT'\n      --AND dd.[DepartmentName] != 'UCH P02 ENDOSCOPY'\n      AND patd.[AgeInYears] >= 18\n      AND (wlef.[IntendedManagement] IN ('*Unspecified', 'Inpatient', 'Inpatient Series', 'Night Admit Series') OR wlef.[IntendedManagement] IS NULL)\n      AND CONVERT(DATE, scufx.[PlannedOperationStartInstant]) = ?\n      AND ((scf.[Classification] = 'Elective') OR (scf.[Classification] = 'Expedited (within 2 weeks on elective list)'))\n      AND ((dd.[DepartmentName] = ?) OR (dd.[DepartmentName] = ?))\n      \n        \"\"\",\n        caboodle_db,\n        params=[date, department1, department2],\n    )\n\n    return data\n\n\n\n\n\n## Create input matrix for model, specifying a date to make a prediction for. The model will return a probability of admission to ICU\n\ndate = \"2022-08-05\"\nto_predict = pd.to_datetime(date)\nlocation = \"gwb\"\nmodel_name, model_version = get_model_details(location)\n\ninput_df = pd.DataFrame(\n    np.array([\"binom\", to_predict, 0, 0])[np.newaxis],\n    columns=[\"model_function\", \"date\", \"icu_counts\", \"noticu_counts\"],\n)\ninput_df.loc[:, \"wkday\"] = (\n    input_df.loc[:, \"date\"].apply(datetime.weekday).astype(\"object\")\n)\ninput_df[\"date\"] = input_df[\"date\"].values.astype(np.float)\ninput_df[\"N\"] = get_elective_cases(to_predict, location)\ninput_df.columns\n\n\npredictor = SKProbabilityPredictorStats(model_name, model_version)\npredictions_df = predictor.predict(input_df)\n\n\npredictions_df[\"probability\"].values\n\n\n\n\n\n# Retrieve the model and the saved lens\n\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\nprint(model.summary())\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n\n    client.download_artifacts(run_id, \"lens\", tmp_dir)\n\n    lens_path = next((tmp_dir / \"lens\").rglob(\"*.pkl\"))\n    with open(lens_path, \"rb\") as f:\n        lens = pickle.load(f)\n\nX_df = lens.transform(input_df)\nX_df__, X_df = dmatrices(expr, X_df, return_type=\"dataframe\")"
  },
  {
    "objectID": "vignettes/icu-tap-emergency-predictions.html",
    "href": "vignettes/icu-tap-emergency-predictions.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "This notebook shows how to use the models I have saved to ML flow, for the purposes of predicting how many non-elective patients will arrive in the ICU in the 24 hours following 9.15 on a given day.\nOne of three locations can be requested: tower, gwb and wms (all lower case). These have different trained models. You need to retrieve the relevant model from ML flow for the location requested. You would do this be setting model_name and model_version to the saved values (as an example for the tower: MODEL__TAP_NONELECTIVE_TOWER__NAME, MODEL__TAP_NONELECTIVE_TOWER__VERSION need to be set; these should eventually be saved as constants in global settings)\nLogic: - Retrieve a ML model which uses simple date parameters (but could one day be more sophisticated) to generate a mean for a poisson distribution - Usep as a parameter to generate a probability distribution over the number of ICU beds needed by non-elective patients, where the maximum number\nNOTES - we DO make predictions for weekends, unlike for the elective taps - but we don’t differentiate between days of week; just a binary indicator for whether it is a weekday or a weekend - we use a lens here which does not really serve any purpose, other than to one-hot encode whether it is a weekend. However, in future, other covariates (eg the number of patients in the hospital currently, the weather) could be added to the model, and the lens is already in place to add scaling functions\nWe agreed to predict flows at 9.15 and 12.30 pm. However, inspection suggested there is minimal difference between these times so I have dropped the second one and only use 9.15 am\n\n\nThe input data to the models takes the form of of one-row dataframe with the following columns; [‘model_function’, ‘date’, ‘count’, ‘wkday’] - model_function - set this to ‘poisson’ - date - use pd.to_datetime to set format eg pd.to_datetime(‘2022-08-08’) - count - set this field to zero [this is an artefact of the lens method] - wkday - an integer for whether it is a weekend (value 0) or a weekday (value 1); set this as shown below\n\n\n\n\nimport pkg_resources\n\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([f\"{i.key}=={i.version}\" for i in installed_packages])\n\n\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\n\nimport os\nimport pickle\nimport tempfile\nfrom pathlib import Path\n\nimport mlflow\nfrom mlflow.tracking import MlflowClient\n\n\nimport urllib\n\nfrom hylib import settings\nfrom hylib.dt import LONDON_TZ\nfrom hymind.predict.base import BaseLensPredictor\nfrom patsy import dmatrices\nfrom scipy.stats import poisson\nfrom sqlalchemy import create_engine\n\n\n\n\n\nmlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n\nmlflow_var = os.getenv(\"HYMIND_REPO_TRACKING_URI\")\nmlflow.set_tracking_uri(mlflow_var)\n\nclient = MlflowClient()\n\n\nMODEL__TAP_NONELECTIVE_TOWER__NAME = \"tap_nonelective_tower\"\nMODEL__TAP_NONELECTIVE_TOWER__VERSION = 2\nMODEL__TAP_NONELECTIVE_GWB__NAME = \"tap_nonelective_gwb\"\nMODEL__TAP_NONELECTIVE_GWB__VERSION = 2\nMODEL__TAP_NONELECTIVE_WMS__NAME = \"tap_nonelective_wms\"\nMODEL__TAP_NONELECTIVE_WMS__VERSION = 2\n\n\ndef get_model_details(location):\n\n    if location == \"tower\":\n        model_name, model_version = (\n            MODEL__TAP_NONELECTIVE_TOWER__NAME,\n            MODEL__TAP_NONELECTIVE_TOWER__VERSION,\n        )\n    elif location == \"gwb\":\n        model_name, model_version = (\n            MODEL__TAP_NONELECTIVE_GWB__NAME,\n            MODEL__TAP_NONELECTIVE_GWB__VERSION,\n        )\n    else:\n        model_name, model_version = (\n            MODEL__TAP_NONELECTIVE_WMS__NAME,\n            MODEL__TAP_NONELECTIVE_WMS__VERSION,\n        )\n    return model_name, model_version\n\n\n\n\n\nclass SKProbabilityPredictorPoisson(BaseLensPredictor):\n    def __init__(self, model_name: str, model_version: int) -> None:\n        super().__init__(model_name, model_version)\n        self.model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n        self.expr = self._load_expr(self.model_info.run_id)\n        self.lens = self._load_lens(self.model_info.run_id)\n\n    @staticmethod\n    def _load_expr(run_id: str):\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp_dir = Path(tmp)\n\n            client.download_artifacts(run_id, \"expr\", tmp_dir)\n\n            expr_path = next((tmp_dir / \"expr\").rglob(\"*.txt\"))\n            with open(expr_path, \"rb\") as f:\n                expr = f.read()\n            expr = str(expr, \"utf-8\")\n\n            return expr\n\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n\n        X_df = self.lens.transform(input_df)\n        X_df__, X_df = dmatrices(self.expr, X_df, return_type=\"dataframe\")\n\n        predictions_set_df = self.model.get_prediction(X_df)\n\n        p = predictions_set_df.summary_frame().iloc[0, 0]\n        N = 11\n\n        predictions_df = pd.DataFrame.from_dict(\n            {\n                \"bed_count\": list(range(0, N + 1)),\n                \"probability\": poisson.pmf(list(range(0, N + 1)), p),\n            }\n        )\n\n        predictions_df[\"predict_dt\"] = datetime.now(LONDON_TZ)\n        predictions_df[\"model_name\"] = self.model_name\n        predictions_df[\"model_version\"] = self.model_version\n        predictions_df[\"run_id\"] = self.model_info.run_id\n\n        return predictions_df\n\n\nclass SKProbabilityPredictorStats(BaseLensPredictor):\n    def __init__(self, model_name: str, model_version: int) -> None:\n        super().__init__(model_name, model_version)\n        self.model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n        self.expr = self._load_expr(self.model_info.run_id)\n        self.lens = self._load_lens(self.model_info.run_id)\n        self.input_df = self._is_weekday(input_df)\n        self.input_df = self._elective_list_gt0(input_df)\n\n    def _is_weekday(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if not input_df.iloc[0, 4] in list(range(0, 5)):\n                raise ValueError(\"Date requested is not a weekday\")\n            return input_df\n\n    def _elective_list_gt0(self, input_df: pd.DataFrame):\n        if input_df.iloc[0, 0] == \"binom\":\n            if input_df.iloc[0, 5] == 0:\n                raise ValueError(\"There are no patients on the elective list\")\n            return input_df\n\n    @staticmethod\n    def _load_expr(run_id: str):\n        with tempfile.TemporaryDirectory() as tmp:\n            tmp_dir = Path(tmp)\n\n            client.download_artifacts(run_id, \"expr\", tmp_dir)\n\n            expr_path = next((tmp_dir / \"expr\").rglob(\"*.txt\"))\n            with open(expr_path, \"rb\") as f:\n                expr = f.read()\n            expr = str(expr, \"utf-8\")\n\n            return expr\n\n    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n\n        X_df = self.lens.transform(input_df)\n        X_df__, X_df = dmatrices(self.expr, X_df, return_type=\"dataframe\")\n\n        predictions_set_df = self.model.get_prediction(X_df)\n        p = predictions_set_df.summary_frame().iloc[0, 0]\n\n        if input_df.iloc[0, 0] == \"binom\":\n\n            N = input_df.iloc[0, 5]\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": binom.pmf(list(range(0, N + 1)), N, p),\n                }\n            )\n\n        else:\n\n            N = 11\n            predictions_df = pd.DataFrame.from_dict(\n                {\n                    \"bed_count\": list(range(0, N + 1)),\n                    \"probability\": poisson.pmf(list(range(0, N + 1)), p),\n                }\n            )\n\n        predictions_df[\"predict_dt\"] = datetime.now(LONDON_TZ)\n        predictions_df[\"model_name\"] = self.model_name\n        predictions_df[\"model_version\"] = self.model_version\n        predictions_df[\"run_id\"] = self.model_info.run_id\n\n        return predictions_df\n\n\n\n\n\n## Create input matrix for model, specifying a date to make a prediction for. The model will return a probability of admission to ICU\n\ndate = \"2022-08-09\"\nto_predict = pd.to_datetime(date)\nlocation = \"tower\"\nmodel_name, model_version = get_model_details(location)\n\ninput_df = pd.DataFrame(\n    np.array([\"poisson\", to_predict, 0])[np.newaxis],\n    columns=[\"model_function\", \"date\", \"count\"],\n)\ninput_df.loc[:, \"wkday\"] = (\n    input_df.loc[:, \"date\"].apply(datetime.weekday).astype(\"object\") <= 4\n)\ninput_df[\"date\"] = input_df[\"date\"].values.astype(np.float)\ninput_df\n\n\nmodel_name, model_version = get_model_details(\"gwb\")\npredictor = SKProbabilityPredictorStats(model_name, model_version)\npredictions_df = predictor.predict(input_df)\n\n\npredictions_df[\"probability\"].values\n\n\n\n\n\nmodel_name, model_version = get_model_details(\"tower\")\n\n\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\nprint(model.summary())\n\n\n# Retrieve the model and the saved lens\n\nmodel = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\nprint(model.summary())\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n\n    client.download_artifacts(run_id, \"lens\", tmp_dir)\n\n    lens_path = next((tmp_dir / \"lens\").rglob(\"*.pkl\"))\n    with open(lens_path, \"rb\") as f:\n        lens = pickle.load(f)\n\nX_df = lens.transform(input_df)\nX_df__, X_df = dmatrices(expr, X_df, return_type=\"dataframe\")"
  },
  {
    "objectID": "vignettes/beds_v3NHNN.html",
    "href": "vignettes/beds_v3NHNN.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "2022-07-03vNHNN\nspecifically for NHNN critical care\nworking notes on merging caboodle info on beds with EMAP\naim is to build a reliable view of bed census\nprogrammatically returns a list of locatoins with Caboodle detail\nbut no testing/quality control\nworks in conjunction with add_caboodle2emap_beds.py\nuse to populate base bed definitions for the tower flow"
  },
  {
    "objectID": "vignettes/beds_v3NHNN.html#set-up-incl-database-connections",
    "href": "vignettes/beds_v3NHNN.html#set-up-incl-database-connections",
    "title": "HYLODE documentation",
    "section": "Set-up incl database connections",
    "text": "Set-up incl database connections\n\nimport os\nimport urllib\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\n\n\ndef emap_db() -> Engine:\n    url = \"postgresql+psycopg2://{}:{}@{}:{}/{}\".format(\n        os.getenv(\"EMAP_DB_USER\"),\n        os.getenv(\"EMAP_DB_PASSWORD\"),\n        os.getenv(\"EMAP_DB_HOST\"),\n        os.getenv(\"EMAP_DB_PORT\"),\n        os.getenv(\"EMAP_DB_NAME\"),\n    )\n    engine = create_engine(\n        url, pool_size=4, max_overflow=6, connect_args={\"connect_timeout\": 120}\n    )\n    return engine\n\n\ndef caboodle_db() -> Engine:\n    db_host = os.getenv(\"CABOODLE_DB_HOST\")\n    db_user = os.getenv(\"CABOODLE_DB_USER\")\n    db_password = os.getenv(\"CABOODLE_DB_PASSWORD\")\n    db_port = os.getenv(\"CABOODLE_DB_PORT\")\n    db_name = os.getenv(\"CABOODLE_DB_NAME\")\n    connection_str = f\"mssql+pyodbc://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}?driver=ODBC+Driver+17+for+SQL+Server\"\n    engine = create_engine(connection_str)\n    return engine\n\n\nemap_engine = emap_db()\ncaboodle_engine = caboodle_db()"
  },
  {
    "objectID": "vignettes/beds_v3NHNN.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "href": "vignettes/beds_v3NHNN.html#reliable-way-of-joining-caboodle-and-emap-bed-level-data",
    "title": "HYLODE documentation",
    "section": "Reliable way of joining Caboodle and EMAP bed level data",
    "text": "Reliable way of joining Caboodle and EMAP bed level data\nFirst load EMAP location table into memory\nDo this with no modifications / transformations other than the join\n\nq = \"\"\"\nSELECT \n    lo.location_id, lo.location_string, lo.department_id, lo.room_id, lo.bed_id, dept.name department, dept.speciality, room.name room\nFROM star.location lo\nLEFT JOIN star.department dept ON lo.department_id = dept.department_id\nLEFT JOIN star.room ON lo.room_id = room.room_id\n\"\"\"\ndfe = pd.read_sql_query(q, emap_engine)\ndfe.head()\n\nAnd load Caboodle DepartmentDim into memory\n\nq = \"\"\"\nSELECT \n DepartmentKey\n,BedEpicId\n,Name\n,DepartmentName\n,RoomName\n,BedName\n,IsBed\n,BedInCensus\n,IsDepartment\n,IsRoom\n,IsCareArea\n,DepartmentExternalName\n,DepartmentSpecialty\n,DepartmentType\n,DepartmentServiceGrouper\n,DepartmentLevelOfCareGrouper\n,LocationName\n,ParentLocationName\n,_CreationInstant\n,_LastUpdatedInstant\nFROM dbo.DepartmentDim\n\"\"\"\ndfc = pd.read_sql_query(q, caboodle_engine)\ndfc.head()\n\nNow join these two tables - don’t attempt joins where there is no room/bed level data (b/c they’re not physical locations - you need a multi-key join - EMAP ‘name’ (department.name) joins on to Caboodle ‘DepartmentName’ - EMAP ‘bed’ (derived by splitting location.location_string) joins on to Caboodle ‘Name’ - drop ‘wait’ beds since these duplicate and block a one-to-one merge - try to be rigorous in pd.merge - indicator=True to allow inspection post merge - validate='one_to_one' to throw an error if duplicates found\nNote - sometimes (in Caboodle) DepartmentName and Name are duplicated so pick the most recently ‘created’\nusing …\nNow load external code\n\nimport importlib\n\nimport add_caboodle2emap_beds\n\nimportlib.reload(add_caboodle2emap_beds)\nfrom add_caboodle2emap_beds import bed_merge\n\n\ndepartments = [\n    # Built from Tower Report 14 Jun 2022\n    # NAME                         # n emap locations\n    # \"UCH T01 ACUTE MEDICAL\",       # 86\n    # \"UCH T01 ENHANCED CARE\",       # 20\n    # \"UCH T03 INTENSIVE CARE\",      # 37\n    # \"UCH T06 HEAD (T06H)\",         # 27\n    # \"UCH T06 CENTRAL (T06C)\",      # 25\n    # \"UCH T06 SOUTH PACU\",          # 22\n    # \"UCH T06 GYNAE (T06G)\",        # 18\n    # \"UCH T07 NORTH (T07N)\",        # 45\n    # \"UCH T07 CV SURGE\",            # 37\n    # \"UCH T07 SOUTH\",               # 33\n    # \"UCH T07 SOUTH (T07S)\",        # 23\n    # \"UCH T07 HDRU\",                # 20\n    # \"UCH T08 NORTH (T08N)\",        # 28\n    # \"UCH T08 SOUTH (T08S)\",        # 25\n    # \"UCH T08S ARCU\",               #  6\n    # \"UCH T09 SOUTH (T09S)\",        # 34\n    # \"UCH T09 NORTH (T09N)\",        # 32\n    # \"UCH T09 CENTRAL (T09C)\",      # 25\n    # \"UCH T10 SOUTH (T10S)\",        # 34\n    # \"UCH T10 NORTH (T10N)\",        # 32\n    # \"UCH T10 MED (T10M)\",          # 16\n    # \"UCH T11 SOUTH (T11S)\",        # 27\n    # \"UCH T11 NORTH (T11N)\",        # 25\n    # \"UCH T11 EAST (T11E)\",         # 16\n    # \"UCH T11 NORTH (T11NO)\",       #  8\n    # \"UCH T12 SOUTH (T12S)\",        # 32\n    # \"UCH T12 NORTH (T12N)\",        # 23\n    # \"UCH T13 SOUTH (T13S)\",        # 31\n    # \"UCH T13 NORTH ONCOLOGY\",      # 26\n    # \"UCH T13 NORTH (T13N)\",        # 26\n    # \"UCH T14 NORTH TRAUMA\",        # 28\n    # \"UCH T14 NORTH (T14N)\",        # 28\n    # \"UCH T14 SOUTH ASU\",           # 22\n    # \"UCH T14 SOUTH (T14S)\",        # 17\n    # \"UCH T15 SOUTH DECANT\",        # 21\n    # \"UCH T15 SOUTH (T15S)\",        # 21\n    # \"UCH T15 NORTH (T15N)\",        # 16\n    # \"UCH T15 NORTH DECANT\",        # 15\n    # \"UCH T16 NORTH (T16N)\",        # 19\n    # \"UCH T16 SOUTH (T16S)\",        # 18\n    # \"UCH T16 SOUTH WINTER\",        # 17\n    # \"GWB L01 ELECTIVE SURG\",       # 37\n    # \"GWB L01 CRITICAL CARE\",       # 12\n    # \"GWB L02 NORTH (L02N)\",        # 19\n    # \"GWB L02 EAST (L02E)\",         # 19\n    # \"GWB L03 NORTH (L03N)\",        # 19\n    # \"GWB L03 EAST (L03E)\",         # 19\n    # \"GWB L04 NORTH (L04N)\",        # 20\n    # \"GWB L04 EAST (L04E)\",         # 17\n    # \"WMS W04 WARD\",                # 28\n    # \"WMS W03 WARD\",                # 27\n    # \"WMS W02 SHORT STAY\",          # 20\n    # \"WMS W01 CRITICAL CARE\",       # 11\n    \"NHNN C0 NCCU\",\n    \"NHNN C1 NCCU\",\n    # \"NHNN C0 EMERGENCY STROKE UNIT\",\n]\n\n\ndfm = bed_merge(df_emap=dfe, df_caboodle=dfc, departments=departments)\ndfm.head()\n\n\ndfm._merge.value_counts()\n\n\ndfm.to_csv(\"beds.tsv\", sep=\"\\t\", index_label=\"local_id\")"
  },
  {
    "objectID": "contributor_guide/pre_commit_hooks.html",
    "href": "contributor_guide/pre_commit_hooks.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "This repository uses the Python package pre-commit to manage pre-commit hooks. Pre-commit hooks are actions which are run automatically, typically on each commit, to perform some common set of tasks. For example, a pre-commit hook might be used to run any code linting automatically before code is committed, ensuring common code quality.\n\n\nFor this repository, we are using pre-commit for a number of purposes:\n\nchecking for secrets being committed accidentally — there is a strict definition of a “secret”; and\nchecking for any large files (over 5 MB) being committed.\ncleaning Jupyter notebooks, which means removing all outputs, execution counts, Python kernels, and, for Google Colaboratory (Colab), stripping out user information.\n\nWe have configured pre-commit to run automatically on every commit. By running on each commit, we ensure that pre-commit will be able to detect all contraventions and keep our repository in a healthy state.\n{note} Pre-commit hooks and Google Colab No pre-commit hooks will be run on Google Colab notebooks pushed directly to GitHub. For security reasons, it is recommended that you manually download your notebook, and commit up locally to ensure pre-commit hooks are run on your changes.\n\n\n\nIn order for pre-commit to run, action is needed to configure it on your system.\n\ninstall the pre-commit package into your Python environment from requirements.txt; and\nrun pre-commit install in your terminal to set up pre-commit to run when code is committed.\n\n\n\n\n{note} Secret detection limitations The `detect-secrets` package does its best to prevent accidental committing of secrets, but it may miss things. Instead, focus on good software development practices! See the [definition of a secret for further information](#definition-of-a-secret-according-to-detect-secrets).\nWe use detect-secrets to check that no secrets are accidentally committed. This hook requires you to generate a baseline file if one is not already present within the root directory. To create the baseline file, run the following at the root of the repository:\ndetect-secrets scan > .secrets.baseline\nNext, audit the baseline that has been generated by running:\ndetect-secrets audit .secrets.baseline\nWhen you run this command, you’ll enter an interactive console. This will present you with a list of high-entropy string and/or anything which could be a secret. It will then ask you to verify whether this is the case. This allows the hook to remember false positives in the future, and alert you to new secrets.\n\n\nThe detect-secrets documentation, as of January 2021, says it works:\n\n…by running periodic diff outputs against heuristically crafted [regular expression] statements, to identify whether any new secret has been committed.\n\nThis means it uses regular expression patterns to scan your code changes for anything that looks like a secret according to the patterns. By definition, there are only a limited number of patterns, so the detect-secrets package cannot detect every conceivable type of secret.\nTo understand what types of secrets will be detected, read the detect-secrets documentation on caveats, and the list of supported plugins. Also, you should use secret variable names with words that will trip the KeywordDetector plugin; see the [DENYLIST variable for the full list of words][detect-secrets-keyword-detector].\n\n\n\nIf pre-commit detects any secrets when you try to create a commit, it will detail what it found and where to go to check the secret.\nIf the detected secret is a false positive, there are two options to resolve this, and prevent your commit from being blocked:\n\ninline allowlisting of false positives (recommended); or\nupdating the .secrets.baseline to include the false positives.\n\nIn either case, if an actual secret is detected (or a combination of actual secrets and false positives), first remove the actual secret. Then following either of these processes.\n\n\nTo exclude a false positive, add a pragma comment such as:\nsecret = \"Password123\"  # pragma: allowlist secret\nor\n#  pragma: allowlist nextline secret\nsecret = \"Password123\"\nIf the detected secret is actually a secret (or other sensitive information), remove the secret and re-commit; there is no need to add any pragma comments.\nIf your commit contains a mixture of false positives and actual secrets, remove the actual secrets first before adding pragma comments to the false positives.\n\n\n\nTo exclude a false positive, you can also update the .secrets.baseline by repeating the same two commands as in the initial setup.\nDuring auditing, if the detected secret is actually a secret (or other sensitive information), remove the secret and re-commit. There is no need to update the .secrets.baseline file in this case.\nIf your commit contains a mixture of false positives and actual secrets, remove the actual secrets first before updating and auditing the .secrets.baseline file.\n\n\n\n\n\nIt may be necessary or useful to keep certain output cells of a Jupyter notebook, for example charts or graphs visualising some set of data. To do this, according to the documentation for the nbstripout package, either:\n\nadd a keep_output tag to the desired cell; or\nadd \"keep_output\": true to the desired cell’s metadata.\n\nYou can access cell tags or metadata in Jupyter by enabling the “Tags” or “Edit Metadata” toolbar (View > Cell Toolbar > Tags; View > Cell Toolbar > Edit Metadata).\nFor the tags approach, enter keep_output in the text field for each desired cell, and press the “Add tag” button. For the metadata approach, press the “Edit Metadata” button on each desired cell, and edit the metadata to look like this:\n{\n  \"keep_output\": true\n}\nThis will tell the hook not to strip the resulting output of the desired cell(s), allowing the output(s) to be committed.\n```{note} Tags and metadata on Google Colab Currently (March 2020) there is no way to add tags and/or metadata to Google Colab notebooks.\nIt’s strongly suggested that you download the Colab as a .ipynb file, and edit tags and/or metadata using Jupyter before committing the code if you want to keep some outputs. ```"
  },
  {
    "objectID": "contributor_guide/updating_gitignore.html",
    "href": "contributor_guide/updating_gitignore.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "The .gitignore used in this repository was created with generic exclusions from gitignore.io, with project-specific exclusions listed afterwards.\nIf you want to add exclusions for new programming languages and/or IDEs, use the first line to recreate the generic exclusions from gitignore.io. Add all other project-specific exclusions afterwards."
  },
  {
    "objectID": "contributor_guide/CONTRIBUTING.html",
    "href": "contributor_guide/CONTRIBUTING.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Footnotes\n\n\nOnly secrets of specific patterns are detected by the pre-commit hooks.↩︎"
  },
  {
    "objectID": "contributor_guide/CODE_OF_CONDUCT.html",
    "href": "contributor_guide/CODE_OF_CONDUCT.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "Contributors to this repository hosted by HYLODE are expected to follow the Contributor Covenant Code of Conduct, and those working within Her Majesty’s Government are also expected to follow the Civil Service Code.\n\n\nContributors working within Her Majesty’s Government must review the Civil Service Code, and are expected to follow it in their contributions.\n\n\n\n\n\nWhere this Code of Conduct says:\n\n“Project”, we mean this hyui GitHub repository;\n“Maintainer”, we mean the HYLODE organisation owners; and\n“Leadership”, we mean both HYLODE organisation owners, line managers, and other leadership within the UCL-HAL.\n\n\n\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project, and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behaviour that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behaviour by participants include:\n\nThe use of sexualised language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behaviour and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviours that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting using an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team at doc@steveharris.me. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct/, and the alphagov Code of Conduct available at https://github.com/alphagov/.github/blob/main/CODE_OF_CONDUCT.md."
  }
]