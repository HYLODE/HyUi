# Deployment


## Setup environment

- Clone the repository on an NHS server `git clone https://github.com/HYLODE/HyUi.git`.
- Configure your `.env` variables using `.env.example` as a template.
- Configure `docker/initialise/.env` variables using `docker/initialise/.env.example.`

### Using the http cache

We use [Varnish](https://varnish-cache.org) as a HTTP cache to speed up some of the slower API calls made by the front end. The default behaviour we follow is to skip the cache and manually identify endpoints which would benefit from caching.

The cache is brought up with other services and is represented in `compose.yml`, which uses `docker/cache/Dockerfile`.
The only change required to utilise caching is within the base `.env` file. The `WEB_API_URL` environment variable should be set to `http://cache:8000` for deployment on the hospital network or `http://cache:8000/mock` for local development with `docker compose`.

Cache configuration is set in `cache/default.vcl` and contains very minimal additions with respect to a basic varnish configuration.

Endpoints can be added to the cache by altering the `Cache-Control` response header. An example is provided in `api/src/api/beds/router.py` under `/mock/baserow/beds/`, where the cache lifetime is set to 300 seconds with `response.headers["Cache-Control"] = "public, max-age=300"`.

The first request made to an uncached API endpoint will not see any speed up - subsequent requests within the time to live (TTL) of an object will use the cached object and will see a speedier response.

Additional configuration is provided for endpoints that absolutely must return live data from the API. In this case, there are two options for skipping the cache.

1. adding `X-Varnish-Nuke:1` to the **request** header will clear the cache for the current endpoint and force a live query of the API
2. Adding `Cache-Control: no-cache` to the request header will instruct Varnish to skip the cache for the requested endpoint

By relying on the setting of cached object TTL via response headers and the skipping (or clearing of) the cache with an additional header, the hope is that developers should not need to interact with `default.vcl` and all changes can be made via [web standards](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control).

The `web` service in `compose.yml` contains a `healthcheck` element that runs the `docker/web/refresh_cache.sh` script at a regular specified time interval. Via this script, endpoints can be called via `curl` with the `X-Varnish-Nuke:1` header to force regular updating of the cached endpoint. This is particularly useful for slow-to-load endpoints that should be available on demand to users.

#### Altering the lifetime of cached objects
By default, endpoints are not cached. This is enforced with response headers set via middleware in `api/src/api/main.py` added as a decorator to the `app` and `mock_router` `FastAPI` instances.. This default is overriden for specified endpoints by altering the response headers as described previously.

TTLs for individual endpoints can be altered by adding `Cache-Control: max-age=<ttl>`, where `<ttl>` represents the desired TTL in seconds.

## Run the Docker Compose services

In the base directory (containing `compose.yml`)

```sh
docker compose --project-name [project name] up --build
```

Ensure that you use a unique name for your `[project name]`. The production service should be named `hyui-prod`.

## Initialise services

This should be done the first time you stand up the services. It populates the Baserow database with the tables and static data required to run the rest of the HyUi services. You will also need to do this when you update your configuration (loading) data for Baserow. That is, you should consider Baserow as a temporary scratch pad, and mapping changes etc. will live in code (see `initialise/src/initialise/bed_defaults.json`). Before you 'redeploy', you must manually delete the tables from Baserow.

You may also want to register the license for the self hosted version. To do this, check that the 'instance id' available on the admin pages is copied from the self-hosted version to the web / hosted version; then regenerate the license key on the hosted version; then copy that license key back to the self-hosted; then add the user to the 'seat' created by that registered version. This enables 'export to JSON' functionality etc.


```sh
./bin/initialise.sh --operation baserow
```

### Recreating defaults in baserow
To be used when updating an existing instance of baserow adjustments to default tables

- loads fresh versions of the default tables including
    - departments

```sh
./bin/initialise.sh --operation recreate_defaults
```

## Stop And Remove Docker Containers

```sh
docker compose --project-name [project name] down
```


## Debugging after deployment in the live environment

Since we don't have root access, then the deployment happens via docker. This means that the debugging must also happen inside a live docker container. The following seems to work.

Ensure that your docker-compose file includes these options for the image that you wish to debug.

If this is your docker-compose config for the **api** service:
```yaml

  api:
    build:
      context: ./src
      dockerfile: Dockerfile.api
...
```
then add these lines
```yaml
    stdin_open: true
    tty: true
```

1. Run `docker-compose up -d --build` as usual.
2. Find the ID of the approprite container via `docker container ps | grep hyui` (e.g. abc123)
3. Use that ID to run `docker attach abc123`. Don't be put off if nothing seems to happen.
4. Whenever the python debugger is triggered (e.g. via a breakpoint `import pdb; pdb.set_trace()`) the container will return control to the terminal above.
5. To exit without killing the container then use `CONTROL-P, CONTROL-Q` instead of `CONTROL-C`

All this via https://blog.lucasferreira.org/howto/2017/06/03/running-pdb-with-docker-and-gunicorn.html with tips for the Python Debugger via https://realpython.com/python-debugging-pdb/.
